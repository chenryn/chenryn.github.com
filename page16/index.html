<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>三斗室</title>
    <meta name="author" content="陈子">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="/assets/themes/twitter/css/pygments/default.css" rel="stylesheet" type="text/css">
    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->
  </head>
  <body>
    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="brand" href="/">三斗室</a>
          <ul class="nav">
      	<li><a href="/tags.html">标签</a></li>
      	<li><a href="/archive.html">归档</a></li>
      	<li><a href="/errata.html">《网站运维技术与实践》勘误</a></li>
      	<li><a href="/projects.html">学习记录</a></li>
      	<li><a href="/categories.html">分类</a></li>
      	<li><a href="/pages.html">Pages</a></li>
            <li><link title="RSS 2.0" type="application/rss+xml" href="http://chenlinux.com/feed.xml" rel="alternate" /><a href="http://chenlinux.com/feed.xml" target="_blank">RSS订阅</a></li>
          </ul>
          <ul class="nav pull-right"><li><a href="/about.html">有关我</a></li></ul>
        </div>
      </div>
    </div>
    <div class="container">
    <div class="row">
      <div class="span7">
<div class="row">
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/19/intro-lm-factor-algorithm" title="cache驻留时间（二、LM-factor算法）" rel="bookmark">cache驻留时间（二、LM-factor算法）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-19 00:00:00 +0800">19 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#cdn-ref" title="cdn" rel="category tag">cdn</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>好吧，满足某人的好奇，花开两朵，各表一枝了。
今天又看到关于LM-factor的另一种说法，特摘录如下：
<img src="/images/uploads/lm-factor.gif" alt="" />
上面这张图来自于《Squid.Definitive.Guide》第七章，对squid的LM-factor算法作出了一个很直观的描述。
请注意这张图的起始时间坐标：Last-Modified，这个是由squid读取的原始web数据所规定的。
然后就是Date，这个是原始数据进入squid的缓冲的时间。
最后就是Expires，这个就是原始数据在squid中的缓冲过期时间。
可以很容易的得出结论，对于LM-factor算法来说，原始数据在squid中的缓冲时间为
(原始数据进入squid的缓冲的时间-原始web数据所规定的Last-Modified时间)<em>percent
所以，我们可以郑重得出结论，在squid的refresh_pattern设置中，percent与Min、Max两个值是完全没有关系！
最后总结一下，对于squid来说，缓冲的数据在cache中的存活时间是这样决定的：
如果有定义refresh_pattern：只要满足以下两个条件之一，缓冲对象过期
缓冲对象在squid的cache缓冲的时间大于refresh_pattern定义的max
缓冲对象在squid的cache缓冲的时间大于(原始数据进入squid的缓冲的时间-原始web数据所规定的Last-Modified时间)</em>percent
用编程语言来描述，就是
if
((CURRENT_DATE-DATE)
elif
((CURRENT_DATE-DATE)/(DATE-LM_DATE)
elif
((CURRENT_DATE-DATE)&gt;max){STABLE}
else{STABLE}</p>
      <a href="/2009/11/19/intro-lm-factor-algorithm" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/18/intro-refresh_pattern" title="cache驻留时间（一、refresh_pattern）" rel="bookmark">cache驻留时间（一、refresh_pattern）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-18 00:00:00 +0800">18 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#cdn-ref" title="cdn" rel="category tag">cdn</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>上一篇举了个缓存时间的事故，现在说说普通情况下影响这个的配置：</p>
<p>首先，refresh_pattern规则仅仅应用到没有明确过时期限的响应。原始服务器能使用Expires头部，或者Cache-Control:max-age指令来指定过时期限。</p>
<p>然后，介绍一下LM-factor算法：</p>
<p>LM-factor=(response age)/(resource age)</p>
<pre><code>响应年龄（即response age）=当前时间-对象进入cache的时间
源年龄（即resource age）=对象进入cache的时间-对象的last_modified
</code></pre>
<p>就好比上篇，因为Last-Modified错误，源年龄变成了三年，而响应年龄相对三年来说太短了，所以LM-factor也就很小很小，永远达不到警戒线……</p>
<p>最后常用刷新选项：</p>
<p>** ignore-reload
该选项导致squid忽略请求里的任何no-cache指令。如果希望内容一进入cache就不删除，直到被主动purge掉为止，可以加上ignore-reload选项,这个我们常用在mp3,wma,wmv,gif之类。</p>
<p>** override-expire
该选项导致squid在检查Expires头部之前，先检查min值。这样，一个非零的min时间让squid返回一个未确认的cache命中，即使该响应准备过期。</p>
<p>** override-lastmod
该选项导致squid在检查LM-factor百分比之前先检查min值。</p>
<p>** reload-into-ims
该选项导致squid以no-cache指令传送确认请求。换句话说，squid在转发请求之前，对该请求增加一个If-Modified-Since头部。注意这点仅仅在目标有Last-Modified时间戳时才能工作。外面进来的请求保留no-cache指令，以便它到达原始服务器。
一般情况可以使用reload-into-ims。它其实是强行控制对象的超时时间，这违反了http协议的精神，但是在带宽较窄的场合，可以提高明显系统相应时间。
以上这段是网上百度的，其实最后一段是废话，squid缓存选项，各个都是有违http协议精神的……</p>
      <a href="/2009/11/18/intro-refresh_pattern" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/18/guess-the-cdn-accel-for-games" title="游戏CDN加速猜想" rel="bookmark">游戏CDN加速猜想</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-18 00:00:00 +0800">18 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#cdn-ref" title="cdn" rel="category tag">cdn</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>没事去国内几个主要的CDN商网站看了看各个的CDN产品说明和解决方案，基本上大同小异（其实连小异我都没怎么找到）。唯一让我惊讶的是蓝汛Chinacache——蓝汛在自己网站上写着“游戏应用加速”，内容如下：</p>
<p>游戏运营商只需要从一点接入ChinaCache的CDN网络，通过ChinaCache的应用加速服务，不论玩家在国内或者国外，无论何时何地，从任何电信运营商接入，玩家的客户端都能够与游戏运营商的服务器之间高速传递游戏数据，从而保证中国南北甚至全球玩家都可以畅快淋漓的参与即时游戏；</p>
<p><img src="http://www.chinacache.com/uploadfile/20080110170409857.jpg" alt="" /></p>
<p>图中的CCR，百度了一下，好像是蓝汛的自动调整流量导向的监控管理平台。</p>
<p>这个图画的不明不白的，对我理解其加速实在是有害无益——话说回来，似乎所有公司的公开解决方案都是这么云里雾里——不如自己去假设：</p>
<p>网游的架构，大体应该是客户端的动作，按照代码生成一个.dat临时文件，并随时上传到服务器端；服务器端定时以及当客户端主动保存时，将该临时文件的数据记录进数据库中存储；等到客户端重新读档（或者掉线回档）时，从数据库中读取最近一次保存的数据。</p>
<p>【以上说法来自我的室友，也是我公司游戏部的同事】</p>
<p>这是上下两个过程，对于往下走的读档流程来说，实在没什么太大的加速价值，因为它是个一锤子买卖（除非是搞成分布式数据库，但这跟CDN什么关系？）；对于往上走的存档流程来说，倒是有些地方可以想想？</p>
<p>关键就在这个.dat临时文件上。我们完全可以让cache服务器缓存这些文件，然后定时上传到上级节点或者源站；还需要开发一个触发器，以便客户端主动存档使用。</p>
<p>不过这些想法，都是建立在尽量传统的CDN理解即内容分发上。而蓝汛一向号称自己有独一无二的动态应用加速手段，从网上找到的GAD白皮书来看，核心还是路由优化而已。恐怕游戏应用加速，也逃脱不了这个范围。
在边写这篇笔记边百度的时候，发现一篇论文《一种CDN中的动态数据存储方案——UbDP》，打算想办法下载全文来看看，其摘要如下：“
CDN对于动态Web应用的加速通常采用数据缓存或复制技术。针对论坛、博客服务提供商等为注册用户提供个人信息发布平台的网站，提出了一种基于用户的数据分割方法：将数据按所属注册用户进行分割，分布到离该用户最近的数据库系统中。将数据库UID操作分散到多个数据库系统，消除了单个数据库系统的I/O瓶颈。
”
相信不会让人失望。</p>
      <a href="/2009/11/18/guess-the-cdn-accel-for-games" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/18/a-strange-accident-of-squid" title="squid一次诡异事故" rel="bookmark">squid一次诡异事故</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-18 00:00:00 +0800">18 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#squid-ref" title="squid" rel="category tag">squid</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>前几天出了一次诡异的事情。某客户在半夜2点钟更新了其网站的内容后，按照刷新规则，squid应该在15分钟内也更新成新内容的。但实际情况却是新网页一刷新没准就变成旧的，一直到5点左右这种现象才算是消失了。</p>
<p>用前段时间写的age测试脚本检查全部服务器，赫然发现有些服务器上的测试文件的age卡在102164686，也就是三年多！</p>
<p>经过整整一个下午的刷新观察，所有的服务器都陆续出现过这种情况，然后不定什么时间age又突然回复正常计数一段时间…等了很久，捕捉到一个现象，就是金华节点的测试age在896的时候，我一刷新，变成102164686了。也就可以认为，这个服务器的age计数在到达15分钟去源站比对文件的时候，突然变成102164686了。</p>
<table>
  <tbody>
    <tr>
      <td>因为之前脚本过滤了其他信息，只显示HTTP1/0</td>
      <td>Age</td>
      <td>Cache三行。于是改手动wget看全部信息。结果无意的刷新几遍后，赫然发现有一次header里的Date居然是2006年！难道是这台机器有问题？确认本机date无误后，我又登陆其他节点几台机器一一试验，都出现这个情况……于是在crontab里执行每5分钟从源站wget一次测试文件，过两天来看看结果，如下所示：</td>
    </tr>
  </tbody>
</table>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@squid1 ~<span class="o">]</span><span class="c"># cat /root/wget.log|awk &#39;/Last/{print $0}&#39; |sort -n |uniq -c</span>
803   Last-Modified: Thu, 12 Nov 2009 05:58:12 GMT
1   Last-Modified: Thu, 24 Aug 2006 00:09:51 GMT
1   Last-Modified: Thu, 24 Aug 2006 00:24:52 GMT
1   Last-Modified: Thu, 24 Aug 2006 00:29:51 GMT
1   Last-Modified: Thu, 24 Aug 2006 00:44:51 GMT
1   Last-Modified: Thu, 24 Aug 2006 00:49:51 GMT
<span class="o">[</span>root@squid1 ~<span class="o">]</span><span class="c"># cat /root/wget.log|awk &#39;/Last/{print $5}&#39; |sort -n |uniq -c</span>
381 2006
803 2009
</code></pre></div>
<p>源站文件的Last-Modified时间居然在变化！而且除了正确的2009年时间不变外，2006年的时间居然是随着时间走的（crontab是5分钟，wget日志里每次Last-Modified的时间也是隔5分钟）……</p>
<p>由此基本确定是客户源站的问题，我的理解是：当cache服务器到时去源站比对时间时，如果碰上源站这会儿时间是2009年，就更新文件并重计age；如果碰上源站这会儿时间是2006年了，那cache比源站还新，自然没法变动了……</p>
<p>但让我不解的是：header信息里Date字段时间变化，还可以说是服务器系统时间不正常，文件的Last-Modified为什么会这样变化呢？！</p>
      <a href="/2009/11/18/a-strange-accident-of-squid" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/17/a-error-page-problem-of-squid" title="squid的一点小问题" rel="bookmark">squid的一点小问题</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-17 00:00:00 +0800">17 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#squid-ref" title="squid" rel="category tag">squid</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>网站运行，出错是必然的。squid提供了一整套多国语言的错误信息页面，放在share/errors/目录下。
但是，让人很尴尬的一点是：squid默认的english错误页面中，居然会公开显示客户源站的IP地址。而有一部分客户，用CDN的目的之一就是要用CDN来分担攻击流量，保护自己。这下可好。生生给暴露出去了。
而附带的简体中文页面中，刚好就没这个信息。真不知道是在讽刺国人攻击性太强，心理太黑暗了么……
不管怎么说，得把这个改掉。最简单的办法，修改english页面，删除掉ERR_CONNECT_FAIL里那个关键的信息。关键的就是下面这一段：</p>
<p>Connection to %I Failed</p>
<p>这个%I，就是源站IP。修改成Connection Failed。显示结果就OK了。
这一次成了，难保下一次别的错误信息里又出什么别的问题。干脆的办法，把错误信息定位到中文包上去。
网上办法多多。</p>
<p>最根本的，在源代码编译的时候，就加上&ndash;enable-err-language=Simplify_Chinese；</p>
<p>最简单的，删除掉English目录，创建一个同名链接链接到Simplify_Chinese目录上；</p>
<p>最实用的，在squid.conf里加上一句配置语句“error_directory /home/squid/share/errors/Simplify_Chinese”，重启服务，即可。</p>
      <a href="/2009/11/17/a-error-page-problem-of-squid" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/16/configure-ssl-support-of-squid" title="squid的SSL配置" rel="bookmark">squid的SSL配置</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-16 00:00:00 +0800">16 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#squid-ref" title="squid" rel="category tag">squid</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>公司新进客户，要求加速它的论坛，比较奇怪的是，整个论坛居然都是https协议的网页。所以得做443端口的配置。
如果只是端口，一个https_port 443就够了。麻烦的地方在证书（之前就有客户死活不肯给证书，于是只能给做个端口转发，顶天了算是路由优化，何苦往CDN里投钱……）。
在拿到证书后，squid.conf里添加这么一句，SSL配置就算是完成了。但测试的时候问题可就多多了</p>
<div class="highlight"><pre><code class="squid">https_port<span class="w"> </span><span class="m">443</span><span class="w"> </span>cert=/test_ssl/server.cer<span class="w"> </span>key=/test_ssl/server.key<span class="w"> </span>defaultsite=bbs.test.com<span class="w"></span>
</code></pre></div>
<p>客户源站在江苏电信，之前的普通静态加速时，为了更好的达到回源效果，所有的网通节点服务器都采用了二级代理的方式，通过BGP回源。  <br />
在按照普通域名走父方式配置完毕后，wget该客户的论坛首页做测试，所有的网通节点返回的状态码倒是200，可首页文件总字节数，只有209！用IE打开一看，赫然是一个“Hello
World！”暴汗……  <br />
改为直接回源后，立马恢复正常。实在不知道这个你好页面是哪里蹦出来的！  <br />
把这个问题交给百哥和谷婶，大家伙都说只要加上一条never_direct allow all就可以强制转发https协议到上级cache服务器就可以了。但我测试的结果很遗憾——209依然——不可行！</p>
<p>和同事讨论没什么办法，大家认为这个应该是父节点要采用的证书应该是另外一种，因为他既要接受子的请求，又要去源站发起请求。目前情况下，只能取消走父配置，统一直接回源而已。然后缓存，刷新时间等等测试一一通过。唉~
[root@squid1 ~]# wget -S -O /dev/null https://bbs.test.com/attachments/month<em>0910/20091014_c30caa02ae844a8dbe58M7PIVoPJPjMh.jpg &ndash;no-check-certificate
&ndash;20:20:28&ndash;
https://bbs.test.com/attachments/month</em>0910/20091014_c30caa02ae844a8dbe58M7PIVoPJPjMh.jpg
Resolving bbs.test.com&hellip; 1.2.3.4
Connecting to bbs.test.com|1.2.3.4|:443&hellip; connected.
WARNING: cannot verify bbs.test.com&rsquo;s certificate, issued by
<code>/C=BE/OU=Domain Validation CA/O=GlobalSign nv-sa/CN=GlobalSign Domain Validation CA':
Unable to locally verify the issuer's authority.
HTTP request sent, awaiting response...
HTTP/1.0 200 OK
Date: Mon, 16 Nov 2009 09:02:07 GMT
Server: Apache/2.2.9 (Unix) DAV/2 mod_ssl/2.2.9 OpenSSL/0.9.8h PHP/5.2.6 mod_apreq2-20051231/2.6.0 mod_perl/2.0.4 Perl/v5.10.0
Last-Modified: Wed, 14 Oct 2009 13:52:59 GMT
ETag: "efc217-1801e-475e57b0924c0"
Accept-Ranges: bytes
Content-Length: 98334
Content-Type: image/jpeg
Age: 1
X-Cache: HIT from cdn.21vianet.com
Connection: keep-alive
Length: 98334 (96K) [image/jpeg]
Saving to: </code>/dev/null&rsquo;
100%[===================================================================================================================&gt;]
98,334
356K/s   in
0.3s
20:20:34 (356 KB/s) - `/dev/null&rsquo; saved [98334/98334]</p>
<p>这里要注意两个地方。</p>
<p>第一，刷新配置的匹配字段不再是^http://bbs.test.com/.<em>而是^https://bbs.test.com/.</em>，不然的话，age值就按之后的默认配置计数了；  <br />
第二，wget测试的时候，必须使用“&ndash;no-check-certificate”参数，否则没法测试；同理，如果是用curl测试，也必须加上“-k”或者“&ndash;insecure”参数。</p>
<p>curl的结果类似下面这样：</p>
<p>[root@squid1 ~]# curl -I https://bbs.test.com/attachments/month_0910/20091014_c30caa02ae844a8dbe58M7PIVoPJPjMh.jpg -k
HTTP/1.0 200 OK
Date: Mon, 16 Nov 2009 09:02:07 GMT
Server: Apache/2.2.9 (Unix) DAV/2 mod_ssl/2.2.9 OpenSSL/0.9.8h
PHP/5.2.6 mod_apreq2-20051231/2.6.0 mod_perl/2.0.4
Perl/v5.10.0
Last-Modified: Wed, 14 Oct 2009 13:52:59 GMT
ETag: &ldquo;efc217-1801e-475e57b0924c0&rdquo;
Accept-Ranges: bytes
Content-Length: 98334
Content-Type: image/jpeg
Age: 187
X-Cache: HIT from cdn.21vianet.com
Connection: close</p>
      <a href="/2009/11/16/configure-ssl-support-of-squid" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/14/anti-hotlinking-in-squid" title="squid防盗链配置" rel="bookmark">squid防盗链配置</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-14 00:00:00 +0800">14 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#squid-ref" title="squid" rel="category tag">squid</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>做网站的，谁愿意自己辛辛苦苦的成果就被别人轻松转载，如果是文字的，一般也就禁鼠标右键，再没什么好办法（当然，名人好打官司另说），但如果是图片，影音的文件，大可以利用http协议的header信息进行控制，这就是大多数web服务器日志要记录的referer。
公司新进一测试客户，就要求CDN方配合做防盗链。</p>
<p>公司自然有规范，直接ctrl+c、ctrl+v就搞定。但这些句子，还是值得细细研究一下的。
相关语句如下：</p>
<div class="highlight"><pre><code class="squid"><span class="k">acl</span><span class="w"> </span>test_domain<span class="w"> </span><span class="k">dstdomain</span><span class="w"> </span>.test.com<span class="w"></span>
<span class="k">acl</span><span class="w"> </span>null_referer<span class="w"> </span><span class="k">referer_regex</span><span class="w"> </span>.<span class="w"></span>
<span class="k">acl</span><span class="w"> </span>right_referer<span class="w"> </span><span class="k">referer_regex</span><span class="w"> </span>-i<span class="w"></span>
^http://test.com<span class="w"> </span>^http://.*.test.com<span class="w"></span>
<span class="k">http_access</span><span class="w"> </span><span class="no">allow</span><span class="w"> </span>test_domain<span class="w"> </span>!null_referer<span class="w"></span>
<span class="k">http_access</span><span class="w"> </span><span class="no">deny</span><span class="w"> </span>test_domain<span class="w"> </span>!right_referer<span class="w"></span>
</code></pre></div>
<p>第一关键点，是第一行的那个“.”，“.”匹配的是除了“n”以外的任何一个字符。那么!null_referer也就是“n”，也就是说第一条access定义的，是允许referer为空行；</p>
<p>第二关键点，是access的“!”，“!”就是非，那么!right_referer定义的就是一切除了test.com以外的域名，也就是说第二条access定义的，是不允许所有其他网站。</p>
<p>这样的结果，也就是只有从test自己的网站，或者直接在浏览器地址栏里输入完整url，才能看到文件（linux上常用的wget、curl，默认的referer也是空，所以也可以。我又试试迅雷，其referer也是空，那么估计下载工具也都是这样）</p>
<p>（比较奇怪的一点是：squid的日志里，空不显示为“ ”，而是“-”，很能迷惑人呀！）</p>
<p>于是我想到新浪和百度呀这些博客之间转来转去的图片，一般都显示一个空图，但点开来（或许还要再刷一次）也一样能看。可见防盗链都是这么做的。</p>
<p>如果真就狠到了连直接url查看也不让，那就把null_referer的定义删除掉，自然也就可以了……</p>
<p>试到这里，发现另一个问题：nagios的监控，一般也是空referer的，如果真这么狠的要求，这个监控也得改了。
因为不管是curl还是wget，都可以伪装referer。
两个的伪装语法分别是：
curl -e &ldquo;http://www.test.com&rdquo; -x $squidip:80 http://www.test.com/test.gif
wget http://www.test.com/test.gif &ndash;refer=&rdquo;http://www.test.com&rdquo; -e &ldquo;http_proxy=$squidip&rdquo;</p>
<p>我对nagios不熟，不知道里面具体是用什么去check的，大概也差不离吧？
最后，像新浪百度这样的盗链显示图片怎么做的？也就是一句话的事，如下：</p>
<div class="highlight"><pre><code class="squid"><span class="k">deny_info</span><span class="w"> </span>http://www.test.com/你盗链啦.gif<span class="w"></span>
right_referer<span class="w"></span>
</code></pre></div>
      <a href="/2009/11/14/anti-hotlinking-in-squid" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/12/awk-variable-3" title="awk变量（三续）" rel="bookmark">awk变量（三续）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-12 00:00:00 +0800">12 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <table>
  <tbody>
    <tr>
      <td>网上闲逛，偶然看到一句统计TCP连接数的命令如右：netstat -n</td>
      <td>awk &lsquo;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&rsquo;</td>
    </tr>
  </tbody>
</table>
<p>要统计TCP连接数，其实用上wc命令，倒不甚难。不过为了熟悉NF的用途，便细细试试这条吧。
先看试验中netstat -n的结果：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl rao<span class="o">]</span><span class="c"># netstat -n |awk &#39;/^tcp/{print $0}&#39;</span>
tcp 0 0 211.151.70.76:80 60.12.137.170:3157 SYN_RECV
tcp 0 0 211.151.70.76:80 117.136.0.184:53476 SYN_RECV
tcp 0 0 211.151.70.76:80 112.193.8.170:4281 SYN_RECV
tcp 0 0 211.151.70.76:80 112.65.48.252:62480 ESTABLISHED
tcp 0 0 211.151.70.76:80 113.205.102.168:3230 ESTABLISHED
tcp 0 0 211.151.70.76:80 198.54.202.250:2714 ESTABLISHED
tcp 0 1370 211.151.70.76:80 222.44.43.141:2070 FIN_WAIT1
tcp 0 0 211.151.70.76:80 220.248.86.74:53112 ESTABLISHED
……（下略）
</code></pre></div>
<p>然后详细打印一下那条命令里NF的每一个变化使用值：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl rao<span class="o">]</span><span class="c"># netstat -n | awk &#39;/^tcp/ {print ++S[$NF],S[$NF],$NF,NF}&#39;</span>
……（上略）
532 532 ESTABLISHED 6
8 8 LAST_ACK 6
533 533 ESTABLISHED 6
534 534 ESTABLISHED 6
33 33 TIME_WAIT 6
535 535 ESTABLISHED 6
536 536 ESTABLISHED 6
</code></pre></div>
<p>也就是利用awk的行处理特性，遍历了所有tcp开头的行。定义出不同状态命名的数组下标，并分别++计数赋值给数组元素。
最后，打印数组S，如下：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@tinysquid2 ~<span class="o">]</span><span class="c"># netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39;</span>
TIME_WAIT 18
FIN_WAIT1 33
FIN_WAIT2 1
ESTABLISHED 508
SYN_RECV 5
LAST_ACK 11
</code></pre></div>
      <a href="/2009/11/12/awk-variable-3" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/12/an-example-of-regular-expression" title="正则表达式一例" rel="bookmark">正则表达式一例</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-12 00:00:00 +0800">12 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#squid-ref" title="squid" rel="category tag">squid</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>前篇只记录了一些正则表达式，没有例子来说明。今天说个简单的例子。</p>
<p>squid中定义refresh_pattern，客户要求有http://www.a.com/b/0/到http://www.a.com/b/20/一共21个目录下的所有文件缓存一定时间。起先随意写了http://www.a.com/b/.*，结果在access.log里发现http://www.a.com/b/下，还有很多除了0-20以外的目录。这就没办法了，只能改。</p>
<p>如果一路写上二十一条refresh，实在麻烦。于是改琢磨正则匹配。</p>
<table>
  <tbody>
    <tr>
      <td>最后结果是http://www.a.com/b/(1{0,1}[0-9]</td>
      <td>20)/.*</td>
    </tr>
  </tbody>
</table>
<pre><code>其中1{0,1}是一部分，{}规定之前的“1”占用0-1位；
[0-9]是第二部分，表示这一位上为0-9的任意一个数字；
两个合在一起，就是(0位)1[0-9]=0-9，（1位）1[0-9]=10-19，也就是0-19；
最后第三部分，单独的20；
全部就是0-20了。
</code></pre>
      <a href="/2009/11/12/an-example-of-regular-expression" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/04/intro-getopts" title="shell技巧——getopts" rel="bookmark">shell技巧——getopts</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-04 00:00:00 +0800">04 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>在写sh脚本的时候，常常需要运行时输入一些数据。之前已经知道用基本的$*，执行的情况，大概就是$0 $1 $2 $3……
那么，那些系统命令里的参数又是怎么做出来的呢？我们自己的脚本如何搞出来$0-$1的效果呢？这就是getopts的作用了。举例如下：</p>
<div class="highlight"><pre><code class="bash"><span class="c">#!/bin/bash</span>
<span class="nb">echo</span> <span class="s2">&quot;OPTIND starts at $OPTIND&quot;</span>
<span class="k">while </span><span class="nb">getopts</span> <span class="s2">&quot;:pq:&quot;</span> optname
<span class="k">do</span>
<span class="k">    case</span> <span class="s2">&quot;$optname&quot;</span> in
    <span class="s2">&quot;p&quot;</span><span class="o">)</span>
        <span class="nb">echo</span> <span class="s2">&quot;Option $optname is specified&quot;</span>
        ;;
    <span class="s2">&quot;q&quot;</span><span class="o">)</span>
        <span class="nb">echo</span> <span class="s2">&quot;Option $optname has value $OPTARG&quot;</span>
        ;;
    <span class="s2">&quot;?&quot;</span><span class="o">)</span>
        <span class="nb">echo</span> <span class="s2">&quot;Unknown option $OPTARG&quot;</span>
        ;;
    <span class="s2">&quot;:&quot;</span><span class="o">)</span>
        <span class="nb">echo</span> <span class="s2">&quot;No argument value for option $OPTARG&quot;</span>
        ;;
    *<span class="o">)</span>
        <span class="c"># Should not occur</span>
        <span class="nb">echo</span> <span class="s2">&quot;Unknown error while processing options&quot;</span>
        ;;
    <span class="k">esac</span>
<span class="k">    </span><span class="nb">echo</span> <span class="s2">&quot;OPTIND is now $OPTIND&quot;</span>
<span class="k">done</span>
</code></pre></div>
<p>在使用getopts命令的时候，shell会自动产生两个变量OPTIND和OPTARG。</p>
<p>OPTIND初始值为1，其含义是下一个待处理的参数的索引。只要存在，getopts命令返回true，所以一般getopts命令使用while循环；</p>
<p>OPTARG是当getopts获取到其期望的参数后存入的位置。而如果不在其期望内，则$optname被设为?并将该意外值存入OPTARG；如果$optname需要拥有具体设置值而实际却没有，则$optname被设为:并将丢失设置值的optname存入OPTARG；</p>
<p>对于$optname，可以用后标:来表示是否需要值；而前标:则表示是否开启静默模式。</p>
      <a href="/2009/11/04/intro-getopts" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/04/dist-shell" title="分布式shell程序" rel="bookmark">分布式shell程序</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-04 00:00:00 +0800">04 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#linux-ref" title="linux" rel="category tag">linux</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>除了用expect+for循环以外，今天偶然看到分布式shell这个概念。随手百度一些资料，放到这里，等过段时间试试~~</p>
<p><a href="http://www.netfort.gr.jp/~dancer/software/dsh.html.en">DSH——dancer&rsquo;s shell / distributed shell</a></p>
<p>开发者是在debian/ubuntu上做的，依赖libdshconfig，如果要部署在其他linux发行版上，还得好好编译一番。
<a href="http://www.theether.org/pssh/">pssh</a> ——这个系列很全，ssh/scp/rsync/nuke/slurp都有。
这个默认下载是rpm包。
<a href="http://sourceforge.net/apps/mediawiki/clusterssh/index.php?title=Main_Page">cssh</a>
这个是用perl编写的，感觉普通使用的时候就像是SecureCRT、XShell这些windows平台上的ssh工具一样标签组管理登陆，但多了一个向组服务器同时发送命令的功能。
还有更多工具，见下：
<a href="http://www.gentoo.org/news/zh_cn/gmn/20080930-newsletter.xml">http://www.gentoo.org/news/zh_cn/gmn/20080930-newsletter.xml</a></p>
      <a href="/2009/11/04/dist-shell" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/04/awk-variable-2" title="awk变量（再续）" rel="bookmark">awk变量（再续）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-04 00:00:00 +0800">04 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>在squid自动配置脚本里，用到了sed的/r把一个文件的内容插入另一个文件。今天看到awk对两个文件的处理方法，要通过不少运算，不怎么方便。不过作为加深对NR和FNR的不同的理解，还是有些作用。
先说下NR和FNR的不同。
在一次awk中，NR是从头计算到尾的，而FNR是每打开一个文件，就重新计算：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;{print NR,FNR,$0}&#39; ts st</span>
1 1 123 456
2 2 abc def
3 3 ABC DEF
4 4 654 321
5 1 123 456
6 2 abc def
7 3 ABC DEF
8 4 654 321
</code></pre></div>
<p>下面转载一个例子：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># cat a</span>
1000 北京市 地级 北京市 北京市
1100 天津市 地级 天津市 天津市
1210 石家庄市 地级 石家庄市 河北省
1210 晋州市 县级 石家庄市 河北省
1243 滦县 县级 唐山市 河北省
1244 滦南县 县级 唐山市 河北省
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># cat b</span>
110000,北京市
120000,天津市
130000,河北省
130131,平山县
130132,元氏县
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{FS=&quot;[|,]&quot;;OFS=&quot;,&quot;}NRFNR{print</span>
<span class="nv">$1</span>,<span class="nv">$2</span>,a<span class="o">[</span><span class="nv">$2</span><span class="o">]}</span><span class="err">&#39;</span> a b
110000,北京市,1000
120000,天津市,1100
130000,河北省,
130131,平山县,
130132,元氏县,
</code></pre></div>
<p>解释：
NRFNR也就是到文件b的时候，打印文件b的第1、2列和之前创建的数组a[北京市]等。</p>
      <a href="/2009/11/04/awk-variable-2" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/04/awk-variable-1" title="awk变量（续）" rel="bookmark">awk变量（续）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-04 00:00:00 +0800">04 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>上回用的是-F（其实如果标准化一点，在BEGIN{}里还可以区分成输入输出的FS和OFS）、NR（当前行数）、NF（当前域数）和$0（当前行全部内容），如果是一般的处理，这些差不多也就够了。</p>
<p>今天再学两个东东，RS和RT。</p>
<p>RS，也就是行分割符；RT，咋翻译，我看了好一会man文档也没搞懂，大概的说，如果RS是单字符的话，RT==RS，如果RS用了正则表达式的话，RT就是当前行RS的内容——也不知道这么说是否准确，目前就理解到这步。注意：RT是GNU awk的扩展功能，所以可能有些平台上不支持。呵呵~~</p>
<p>说实话，理解这个RS颇是花了我不少脑细胞去想象。直到看到一个网页，大概意思是这样：
假如test内容是：
123 456
abc def
ABC DEF
654 321
那么对于类UNIX系统来说，test文件内容其实是123 456\nabc def\nABC DEF\n654 321
awk默认的RS，就是&rdquo;\n&rdquo;（默认FS是&rdquo; &ldquo;和&rdquo;\t&rdquo;即tab），每碰见一个RS，awk就停下来，输入space处理。假如一直没有RS，就输完全部文件为止。
如果在BEGIN{}里另外定义RS的话，要注意的是，这个时候&rdquo;\n&rdquo;还不会成为字符出现，而是自动转为默认的FS。</p>
<p>对test的实验过程如下：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># cat test</span>
123 456
abc def
ABC DEF
654 321
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;{print $1}&#39; test</span>
123
abc
ABC
654<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;}{print $1}&#39; test</span>
123
DEF
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot; &quot;}{print $1}&#39; test</span>
123
DEF
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot;\n&quot;}{print $1}&#39; test</span>
123 456
DEF
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot;abc&quot;}{print $1}&#39; test</span>
123 456
DEF
654 321
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot;\t&quot;}{print $1}&#39; test</span>
123 456
abc def
DEF
654 321
</code></pre></div>
<p>我是似乎明白了，不知道路过我博客的同仁们明白了么？</p>
<p>然后说RT，还是用实验来证明吧：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot;\t&quot;}{print $1,RT}&#39; ts</span>
123 456
abc def
ABC
DEF
654 321
<span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot;\t&quot;}{print $1,RS}&#39; ts</span>
123 456
abc def
ABC
DEF
654 321
ABC
</code></pre></div>
<p>对了，还记得上回取上一行用的办法么？我再试试：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@raocl ~<span class="o">]</span><span class="c"># awk &#39;BEGIN{RS=&quot;ABC&quot;;FS=&quot;\t&quot;}{print $1,x}{x=RT}&#39; ts</span>
123 456
abc def
DEF
654 321
ABC
</code></pre></div>
<p>也是打印出来上一行的RT了。
这个都是同一的字符做RS，下面转载一个复杂的正则匹配RS的例子：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@mip blog<span class="o">]</span><span class="c"># cat TR_file</span>
Sun Jan 2 07:42:56 2000
Database mounted in Exclusive Mode
Completed: ALTER DATABASE MOUNT
Sun Jan 2 07:42:56 2000
Database tested in Exclusive Mode
Completed: ALTER DATABASE MOUNT
abc Jan 2 12:42:56 2000
Database mounted in Exclusive Mode
Completed: ALTER DATABASE MOUNT
Sun Jan 2 23:00:00 2009
Database mounted in Exclusive Mode
Completed: ALTER DATABASE MOUNT
<span class="o">[</span>root@mip blog<span class="o">]</span><span class="c"># awk -v RS=&#39;[[:alpha:]]+ [[:alpha:]]+ [0-9][0-9][0-9]:[0-9][0-9]:[0-9][0-9]&#39; &#39;$0~/mounted/{print s}{s=RT}&#39;</span>
RT_file
Sun Jan 2 07:42:56
abc Jan 2 12:42:56
Sun Jan 2 23:00:00
</code></pre></div>
      <a href="/2009/11/04/awk-variable-1" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/04/awk-built-in-function" title="awk内置函数" rel="bookmark">awk内置函数</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-04 00:00:00 +0800">04 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>前几篇说awk变量，今天说函数。</p>
<p>看到内置变量的中文翻译如下：</p>
<p>ARGC命令行参数个数 AGRV命令行参数排列 ENVIRON支持队列中系统环境变量的使用 FILENAME浏览文件名
FNR浏览文件的记录数 FS输入域分隔符 NF浏览记录的域个数 NR已读的记录数 OFS输出域分隔符 ORS输出记录分隔符
RS控制记录分隔符</p>
<p>index(s,t) 返回s中字符串t的第一位置
[root@raocl ~]# awk &lsquo;BEGIN {print index(&ldquo;Sunny&rdquo;,&rdquo;ny&rdquo;)}&rsquo;
4</p>
<p>length(s) 返回s的长度
[root@raocl ~]# awk &lsquo;BEGIN {print length(&ldquo;Sunny&rdquo;)}&rsquo;
5</p>
<p>match(s,r) 测试s是否包含匹配r的字符串，默认带两个变量RSTART、RLENGTH，分别是开始位置和占用长度
[root@raocl ~]# echo 12|awk &lsquo;$1=&rdquo;J.Lulu&rdquo;{print match($1,&rdquo;u&rdquo;),RSTART,RLENGTH}&rsquo;
4 4 1</p>
<p>split(s,a,fs) 以fs为分隔符将s分割输入数组a
[root@raocl ~]# awk &lsquo;BEGIN {print split(&ldquo;12#345#6789&rdquo;,myarray,&rdquo;#&rdquo;),myarray[2]}&rsquo;
3 345</p>
<p>substr(s,p) 返回字符串s中从p开始的后缀部分</p>
<p>substr(s,p,n) 返回字符串s中从p开始长度为n的后缀部分
[root@raocl ~]# echo abcdefg|awk &lsquo;{print substr($0,1,length($0)-4)}&rsquo;
abc</p>
<p>gsub(r,s,t) 在t中用s替代r（不写t就是$0）
（附：sub()函数只替换第一次出现的位置；另，sub/gsub修改字符串，而substr是生成子串，不修改原串）
[root@raocl ~]# echo abc|awk &lsquo;gsub(/ab/,&rdquo;12&rdquo;,$0)&rsquo;
12c</p>
      <a href="/2009/11/04/awk-built-in-function" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/zz-three-ways-to-keep-process-running-in-the-background" title="让进程在后台可靠运行的几种方法（转）" rel="bookmark">让进程在后台可靠运行的几种方法（转）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <ul>
  <li>nohup
nohup 无疑是我们首先想到的办法。顾名思义，nohup 的用途就是让提交的命令忽略 hangup 信号。让我们先来看一下nohup 的帮助信息：
  NOHUP(1)
  User
  Commands
  NOHUP(1)
  NAME
  nohup - run a command immune to hangups, with output to a
  non-tty
  SYNOPSIS
  nohup COMMAND [ARG]&hellip;
  nohup OPTION
  DESCRIPTION
  Run COMMAND, ignoring hangup signals.
  &ndash;help display this help and exit
  &ndash;version
  output version information and exit
可见，nohup 的使用是十分方便的，只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到nohup.out文件中。一般我们可在结尾加上&rdquo;&amp;&rdquo;来将命令同时放入后台运行，也可用&rdquo;&gt; filename 2&gt;&amp;1&rdquo;来更改缺省的重定向文件名。</li>
</ul>
<p>** nohup 示例</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># nohup ping www.ibm.com &amp;</span>
<span class="o">[</span>1<span class="o">]</span> 3059 nohup: appending output to <span class="sb">`</span>nohup.out<span class="err">&#39;</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># ps -ef |grep 3059</span>
root 3059 984  0 21:06 pts/3 00:00:00 ping www.ibm.com
root 3067 984  0 21:06 pts/3 00:00:00 grep 3059
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c">#</span>
</code></pre></div>
<p>** hangup 名称的来由</p>
<p>在 Unix 的早期版本中，每个终端都会通过 modem 和系统通讯。当用户 logout 时，modem 就会挂断（hangup）电话。 同理，当 modem 断开连接时，就会给终端发送 hangup 信号来通知其关闭所有子进程。</p>
<ul>
  <li>setsid</li>
</ul>
<p>nohup 无疑能通过忽略 HUP 信号来使我们的进程避免中途被中断，但如果我们换个角度思考，如果我们的进程不属于接受HUP信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。让我们先来看一下 setsid的帮助信息：
    SETSID(8)
    Linux Programmer’s
    Manual
    SETSID(8)
    NAME
    setsid - run a program in a new session
    SYNOPSIS
    setsid program [ arg &hellip; ]
    DESCRIPTION
    setsid runs a program in a new session.
可见 setsid 的使用也是非常方便的，也只需在要处理的命令前加上 setsid 即可。</p>
<p>** setsid 示例</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># nohup ping www.ibm.com &amp;</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># setsid ping www.ibm.com</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># ps -ef |grep www.ibm.com</span>
root 31094 1  0 07:28 ? 00:00:00 ping www.ibm.com
root 31102 29217  0 07:29 pts/4 00:00:00 grep www.ibm.com
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c">#</span>
</code></pre></div>
<p>值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。</p>
<ul>
  <li>&amp;</li>
</ul>
<p>这里还有一个关于 subshell 的小技巧。我们知道，将一个或多个命名包含在“()”中就能让这些命令在子 shell中运行中，从而扩展出很多有趣的功能，我们现在要讨论的就是其中之一。
当我们将&rdquo;&amp;&rdquo;也放入“()”内之后，我们就会发现所提交的作业并不在作业列表中，也就是说，是无法通过jobs来查看的。让我们来看看为什么这样就能躲过HUP 信号的影响吧。</p>
<p>** subshell示例</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># nohup ping www.ibm.com &amp;</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># (ping www.ibm.com &amp;)</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># ps -ef |grep www.ibm.com</span>
root 16270 1  0 14:13 pts/4 00:00:00 ping www.ibm.com
root 16278 15362  0 14:13 pts/4 00:00:00 grep www.ibm.com
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c">#</span>
</code></pre></div>
<p>从上例中可以看出，新提交的进程的父 ID（PPID）为1（init 进程的 PID），并不是当前终端的进程ID。因此并不属于当前终端的子进程，从而也就不会受到当前终端的 HUP 信号的影响了。</p>
<ul>
  <li>disown</li>
</ul>
<p>场景：  <br />
我们已经知道，如果事先在命令前加上 nohup 或者 setsid 就可以避免 HUP 信号的影响。但是如果我们未加任何处理就已经提交了命令，该如何补救才能让它避免 HUP 信号的影响呢？</p>
<p>解决方法：  <br />
这时想加 nohup 或者 setsid 已经为时已晚，只能通过作业调度和 disown 来解决这个问题了。让我们来看一下disown 的帮助信息：
    disown [-ar] [-h] [jobspec &hellip;]
    Without options, each jobspec is
    removed  from
    the  table
    of
    active
    jobs.
    If  the -h option is given, each jobspec is not
    removed from the table, but is marked so that  SIGHUP is not
    sent  to the job if the shell receives a SIGHUP. If no jobspec
    is present, and neither the -a nor the -r option is supplied,
    the  current job  is used.  If no jobspec is supplied, the
    -a option means to remove or mark all jobs; the -r option without
    a  jobspec argument  restricts operation to running jobs.  The
    return value is 0 unless a jobspec does not specify a valid job.</p>
<p>可以看出，我们可以用如下方式来达成我们的目的。</p>
<p>** 用disown -h jobspec 来使某个作业忽略HUP信号。
** 用disown -ah 来使所有的作业都忽略HUP信号。
** 用disown -rh 来使正在运行的作业忽略HUP信号。</p>
<p>需要注意的是，当使用过 disown 之后，会将把目标作业从作业列表中移除，我们将不能再使用jobs来查看它，但是依然能够用ps -ef查找到它。</p>
<p>但是还有一个问题，这种方法的操作对象是作业，如果我们在运行命令时在结尾加了&rdquo;&amp;&rdquo;来使它成为一个作业并在后台运行，那么就万事大吉了，我们可以通过jobs命令来得到所有作业的列表。但是如果并没有把当前命令作为作业来运行，如何才能得到它的作业号呢？答案就是用CTRL-z（按住Ctrl键的同时按住z键）了！</p>
<p>CTRL-z 的用途就是将当前进程挂起（Suspend），然后我们就可以用jobs命令来查询它的作业号，再用bg jobspec来将它放入后台并继续运行。需要注意的是，如果挂起会影响当前进程的运行结果，请慎用此方法。</p>
<p>** disown示例1</p>
<p>（如果提交命令时已经用“&amp;amp;”将命令放入后台运行，则可以直接使用“disown”）</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># cp -r testLargeFile largeFile &amp;</span>
<span class="o">[</span>1<span class="o">]</span> 4825
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># jobs</span>
<span class="o">[</span>1<span class="o">]</span>+
Running cp -i -r testLargeFile largeFile &amp;
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># disown -h %1</span>
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># ps -ef |grep largeFile</span>
root 4825 968  1 09:46 pts/4 00:00:00 cp -i -r testLargeFile largeFile
root 4853 968  0 09:46 pts/4 00:00:00 grep largeFile
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c">#</span>
</code></pre></div>
<p>** disown 示例2</p>
<p>（如果提交命令时未使用“&amp;amp;”将命令放入后台运行，可使用 CTRL-z和“bg”将其放入后台，再使用“disown”）</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># cp -r testLargeFile largeFile2</span>
<span class="o">[</span>1<span class="o">]</span>+ Stopped
cp -i -r testLargeFile largeFile2
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># bg %1</span>
<span class="o">[</span>1<span class="o">]</span>+ cp -i -r testLargeFile largeFile2 &amp;
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># jobs</span>
<span class="o">[</span>1<span class="o">]</span>+ Running
cp -i -r testLargeFile largeFile2 &amp;
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># disown -h %1</span>
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c"># ps -ef |grep largeFile2</span>
root 5790  5577  1 10:04 pts/3 00:00:00 cp -i -r testLargeFile largeFile2
root 5824  5577  0 10:05 pts/3 00:00:00 grep largeFile2
<span class="o">[</span>root@pvcent107 build<span class="o">]</span><span class="c">#</span>
</code></pre></div>
<p>灵活运用 CTRL-z</p>
<p>在我们的日常工作中，我们可以用 CTRL-z 来将当前进程挂起到后台暂停运行，执行一些别的操作，然后再用 fg来将挂起的进程重新放回前台（也可用 bg来将挂起的进程放在后台）继续运行。这样我们就可以在一个终端内灵活切换运行多个任务，这一点在调试代码时尤为有用。因为将代码编辑器挂起到后台再重新放回时，光标定位仍然停留在上次挂起时的位置，避免了重新定位的麻烦。</p>
<ul>
  <li>screen</li>
</ul>
<p>场景：  <br />
我们已经知道了如何让进程免受 HUP 信号的影响，但是如果有大量这种命令需要在稳定的后台里运行，如何避免对每条命令都做这样的操作呢？
解决方法：  <br />
此时最方便的方法就是 screen 了。简单的说，screen 提供了 ANSI/VT100的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen的参数很多，具有很强大的功能，我们在此仅介绍其常用功能以及简要分析一下为什么使用 screen 能够避免 HUP信号的影响。我们先看一下 screen 的帮助信息：
    SCREEN(1)
    SCREEN(1)
    NAME
    screen - screen manager with VT100/ANSI terminal emulation
    SYNOPSIS
    screen [ -options ] [ cmd [ args ] ]
    screen -r [[pid.]tty[.host]]
    screen -r sessionowner/[[pid.]tty[.host]]
    DESCRIPTION
    Screen is a full-screen window manager that multiplexes a physical
    terminal between several processes (typically interactive  shells).
    Each virtual terminal provides the functions of a DEC VT100 terminal
    and, in addition, several control functions from the ISO 6429 (ECMA 48,
    ANSI X3.64)  and ISO 2022 standards (e.g. insert/delete line and
    support for multiple character sets).
    There is a scrollback  history buffer  for  each
    virtual terminal and a copy-and-paste mechanism that
    allows moving text regions between windows.</p>
<p>使用 screen 很方便，有以下几个常用选项：</p>
<p>** 用screen -dmS session name 来建立一个处于断开模式下的会话（并指定其会话名）。
** 用screen -list 来列出所有会话。
** 用screen -r session name 来重新连接指定会话。
** 用快捷键CTRL-a d 来暂时断开当前会话。</p>
<p>** screen 示例</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># screen -dmS Urumchi</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># screen -list</span>
There is a screen on: 12842.Urumchi <span class="o">(</span>Detached<span class="o">)</span>
1 Socket in /tmp/screens/S-root.
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># screen -r</span>
Urumchi
</code></pre></div>
<p>当我们用“-r”连接到 screen 会话后，我们就可以在这个伪终端里面为所欲为，再也不用担心 HUP 信号会对我们的进程造成影响，也不用给每个命令前都加上“nohup”或者“setsid”了。这是为什么呢？让我来看一下下面两个例子吧。
*** 未使用 screen 时新进程的进程树</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># ping www.google.com &amp;amp;amp;</span>
<span class="o">[</span>1<span class="o">]</span> 9499
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># pstree -H 9499</span>
init─┬─Xvnc
├─acpid
├─atd
├─2*<span class="o">[</span>sendmail<span class="o">]</span>
├─sshd─┬─sshd───bash───pstree
│
└─sshd───bash───ping
</code></pre></div>
<p>我们可以看出，未使用 screen 时我们所处的 bash 是 sshd 的子进程，当 ssh 断开连接时，HUP信号自然会影响到它下面的所有子进程（包括我们新建立的 ping 进程）。
*** 使用了 screen 后新进程的进程树</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># screen -r Urumchi</span>
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># ping www.ibm.com &amp;amp;amp;</span>
<span class="o">[</span>1<span class="o">]</span> 9488
<span class="o">[</span>root@pvcent107 ~<span class="o">]</span><span class="c"># pstree -H 9488</span>
init─┬─Xvnc
├─acpid
├─atd
├─screen───bash───ping
├─2*<span class="o">[</span>sendmail<span class="o">]</span>
</code></pre></div>
<p>而使用了 screen 后就不同了，此时 bash 是 screen 的子进程，而 screen 是init（PID为1）的子进程。那么当 ssh 断开连接时，HUP 信号自然不会影响到 screen 下面的子进程了。</p>
<ul>
  <li>总结</li>
</ul>
<p>现在几种方法已经介绍完毕，我们可以根据不同的场景来选择不同的方案。nohup/setsid 无疑是临时需要时最方便的方法，disown能帮助我们来事后补救当前已经在运行了的作业，而 screen 则是在大批量操作时不二的选择了。
* 参考资料
** “系统管理员工具包：进程管理技巧”（developerWorks 中国，2006 年 5 月）介绍了 Linux进程管理的更多技巧。
** “Linux 技巧：使用 screen 管理你的远程会话”（developerWorks 中国，2007 年 7 月）介绍了screen 的更多技巧。
* 在 developerWorks 中国网站 Linux 专区中学习更多 Linux 方面的知识。
* 关于作者
申毅，IBM 中国软件开发中心 WebSphere Portal 部门软件工程师。
<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/index.html">原文地址</a></p>
      <a href="/2009/11/03/zz-three-ways-to-keep-process-running-in-the-background" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/zz-linux-performance-command-tools" title="Linux命令行系统性能检测工具(转)" rel="bookmark">Linux命令行系统性能检测工具(转)</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#linux-ref" title="linux" rel="category tag">linux</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>※注：下面附图的命令输出信息，以红旗DC Server 5.0 for x86 Sp1为基础平台，可能在不同的操作系统或核心版本有较大区别，对比时请留意。</p>
<ul>
  <li>uptime</li>
</ul>
<p>Uptime 命令的显示结果包括服务器已经运行了多长时间，有多少登陆用户和对服务器性能的总体评估（load average）。load average值分别记录了上个1分钟，5分钟和15分钟间隔的负载情况，load average不是一个百分比，而是在队列中等待执行的进程的数量。如果进程要求CPU时间被阻塞（意味着CPU没有时间处理它），load average值将增加。另一方面，如果每个进程都可以立刻得到访问CPU的时间，这个值将减少。
kernel下的load average的最佳值是1，这说明每个进程都可以立刻被CPU处理，当然，更低不会有问题，只说明浪费了一部分的资源。但在不同的系统间这个值也是不同的，例如一个单CPU的工作站，load average为1或者2都是可以接受的，而在一个多CPU的系统中这个值应除以物理CPU的个数，假设CPU个数为4，而load average为8或者10，那结果也是在2多点而已。
<a href="http://www.linuxfly.org/attachment/1169008391_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008391_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a>
你可以使用uptime判断一个性能问题是出现在服务器上还是网络上。例如，如果一个网络应用运行性能不理想，运行uptime检查系统负载是否比较高，如果不是这个问题更可能出现在你的网络上。</p>
<ul>
  <li>top</li>
</ul>
<p>Top命令显示了实际CPU使用情况，默认情况下，它显示了服务器上占用CPU的任务信息并且每5秒钟刷新一次。你可以通过多种方式分类它们，包括PID、时间和内存使用情况。
<a href="http://www.linuxfly.org/attachment/1169008444_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008444_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a>
下面是输出值的介绍：</p>
<pre><code>PID：进程标识
USER；进程所有者的用户名
PRI：进程的优先级
NI：nice级别
SIZE：进程占用的内存数量（代码＋数据＋堆栈）
RSS；进程使用的物理内存数量
SHARE；该进程和其他进程共享内存的数量
STAT：进程的状态：S＝休眠状态，R＝运行状态，T＝停止状态，D＝中断休眠状态，Z＝僵尸状态
%CPU：共享的CPU使用
%MEM；共享的物理内存
TIME：进程占用CPU的时间
COMMAND：启动任务的命令行（包括参数）
</code></pre>
<p>** 进程的优先级和nice级别
进程优先级是一个决定进程被CPU执行优先顺序的参数，内核会根据需要调整这个值。Nice值是一个对优先权的限制。进程优先级的值不能低于nice值。（nice值越低优先级越高）
进程优先级是无法去手动改变的，只有通过改变nice值去间接的调整进程优先级。如果一个进程运行的太慢了，你可以通过指定一个较低的nice值去为它分配更多的CPU资源。当然，这意味着其他的一些进程将被分配更少的CPU资源，运行更慢一些。Linux支持nice值的范围是19（低优先级）到-20（高优先级），默认的值是0。如果需要改变一个进程的nice值为负数（高优先级），必须使用su命令登陆到root用户。下面是一些调整nice值的命令示例，</p>
<p>以nice值-5开始程序xyz</p>
<h1 id="nice-n--5-xyz">nice –n -5 xyz</h1>
<p>改变已经运行的程序的nice值</p>
<h1 id="renice-level-pid">renice level pid</h1>
<p>将pid为2500的进程的nice值改为10</p>
<h1 id="renice-10-2500">renice 10 2500</h1>
<p>** 僵尸进程
当一个进程被结束，在它结束之前通常需要用一些时间去完成所有的任务（比如关闭打开的文件），在一个很短的时间里，这个进程的状态为僵尸状态。在进程完成所有关闭任务之后，会向父进程提交它关闭的信息。有些情况下，一个僵尸进程不能关闭它自己，这时这个进程状态就为z（zombie）。不能使用kill命令杀死僵尸进程，因为它已经标志为“dead”。如果你无法摆脱一个僵尸进程，你可以杀死它的父进程，这个僵尸进程也就消失了。然而，如果父进程是init进程，你不能杀死init进程，因为init是一个重要的系统进程，这种情况下你只能通过一次重新启动服务器来摆脱僵尸进程。也必须分析应用为什么会导致僵死？</p>
<ul>
  <li>iostat</li>
</ul>
<p>iostat是sysstat包的一部分。Iostat显示自系统启动后的平均CPU时间（与uptime类似），它也可以显示磁盘子系统的使用情况，iostat可以用来监测CPU利用率和磁盘利用率。
<a href="http://www.linuxfly.org/attachment/1169008526_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008526_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a></p>
<p>CPU利用率分四个部分：</p>
<pre><code>%user：user level（应用）的CPU占用率情况
%nice：加入nice优先级的user level的CPU占用率情况
%sys：system level（内核）的CPU占用情况
%idle：空闲的CPU资源情况
</code></pre>
<p>磁盘占用率有下面几个部分：</p>
<pre><code>Device：块设备名
Tps：设备每秒进行传输的数量（每秒的I/O请求）。多个单独的I/O请求可以被组成一个传输操作，因为一个传输操作可以是不同的容量。
Blk_read/s, Blk_wrtn/s：该设备每秒读写的块的数量。块可能为不同的容量。
Blk_read, Blk_wrtn：自系统启动以来读写的块设备的总量。
</code></pre>
<p>块的大小
块可能为不同的容量。块的大小一般为1024、2048、4048byte。可通过tune2fs或dumpe2fs获得：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@rfgz ~<span class="o">]</span><span class="c"># tune2fs -l /dev/hda1|grep &#39;Block size&#39;</span>
Block size: 4096
<span class="o">[</span>root@rfgz ~<span class="o">]</span><span class="c"># dumpe2fs -h /dev/hda1|grep &#39;Block size&#39;</span>
dumpe2fs 1.35 <span class="o">(</span>28-Feb-2004<span class="o">)</span>
Block size: 4096
</code></pre></div>
<ul>
  <li>Vmstat
Vmstat命令提供了对进程、内存、页面I/O块和CPU等信息的监控，vmstat可以显示检测结果的平均值或者取样值，取样模式可以提供一个取样时间段内不同频率的监测结果。
<a href="http://www.linuxfly.org/attachment/1169008594_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008594_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a></li>
</ul>
<p>注：在取样模式中需要考虑在数据收集中可能出现的误差，将取样频率设为比较低的值可以尽可能的减小误差的影响。
下面介绍一下各列的含义</p>
<pre><code>·process（procs）
r：等待运行时间的进程数量
b：处在不可中断睡眠状态的进程
w：被交换出去但是仍然可以运行的进程，这个值是计算出来的
·memoryswpd：虚拟内存的数量
free：空闲内存的数量
buff：用做缓冲区的内存数量
·swap
si：从硬盘交换来的数量
so：交换到硬盘去的数量
·IO
bi：向一个块设备输出的块数量
bo：从一个块设备接受的块数量
·system
in：每秒发生的中断数量， 包括时钟
cs：每秒发生的context switches的数量
·cpu(整个cpu运行时间的百分比)
us：非内核代码运行的时间（用户时间，包括nice时间）
sy：内核代码运行的时间（系统时间）
id：空闲时间，在Linux 2.5.41之前的内核版本中，这个值包括I/O等待时间；
wa：等待I/O操作的时间，在Linux 2.5.41之前的内核版本中这个值为0
</code></pre>
<p>Vmstat命令提供了大量的附加参数，下面列举几个十分有用的参数：</p>
<pre><code>·m：显示内核的内存利用率
·a：显示内存页面信息，包括活跃和不活跃的内存页面
·n：显示报头行，这个参数在使用取样模式并将命令结果输出到一个文件时非常有用。例如root#vmstat –n 2 10以2秒的频率显示10输出结果
·当使用-p {分区}时，vmstat提供对I/O结果的统计
</code></pre>
<p><a href="http://www.linuxfly.org/attachment/1169008668_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008668_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a>
<a href="http://www.linuxfly.org/attachment/1169008747_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008747_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a></p>
<ul>
  <li>ps和pstree</li>
</ul>
<p>ps 和pstree命令是系统分析最常用的基本命令，ps命令提供了一个正在运行的进程的列表，列出进程的数量取决于命令所附加的参数。例如ps –A 命令列出所有进程和它们相应的进程ID（PID），进程的PID是使用其他一些工具之前所必须了解的，例如pmap或者renice。
在运行java应用的系统上，ps –A 命令的输出很容易就会超过屏幕的显示范围，这样就很难得到所有进程的完整信息。这时，使用pstree命令可以以树状结构来显示所有的进程信息并且可以整合子进程的信息。Pstree命令对分析进程的来源十分有用。
<a href="http://www.linuxfly.org/attachment/1169008793_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008793_0.jpg" border="0" alt="点击在新窗口中浏览此图片" /></a></p>
<ul>
  <li>Numastat</li>
</ul>
<p>随着NUMA架构的不断发展，例如eServer xSeries 445及其后续产品eServer xSeries 460，现在NUMA架构已经成为了企业级数据中心的主流。然而，NUMA架构在性能调优方面面临了新的挑战，例如内存分配的问题在NUMA系统之前并没人感兴趣，而Numastat命令提供了一个监测NUMA架构的工具。Numastat命令提供了本地内存与远程内存使用情况的对比和各个节点的内存使用情况。Numa_miss列显示分配失败的本地内存，numa_foreign列显示分配远程内存（访问速度慢）信息，过多的调用远程内存将增加系统的延迟从而影响整个系统的性能。使运行在一个节点上的进程都访问本地内存将极大的改善系统的性能。
<a href="http://www.linuxfly.org/attachment/1169008814_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008814_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a></p>
<ul>
  <li>sar</li>
</ul>
<p>sar程序也是sysstat安装包的一部分。sar命令用于收集、报告和保存系统的信息。Sar命令由三个应用组成：sar，用与显示数据；sa1和sa2，用于收集和存储数据。默认情况下，系统会在crontab中加入自动收集和分析的操作：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@rfgz ~<span class="o">]</span><span class="c"># cat /etc/cron.d/sysstat</span>
<span class="c"># run system activity accounting tool every 10 minutes</span>
*/10 * * * * root /usr/lib/sa/sa1 1 1
<span class="c"># generate a daily summary of process accounting at 23:53</span>
53 23 * * * root /usr/lib/sa/sa2 -A
</code></pre></div>
<p>sar命令所生成的数据保存在/var/log/sa/目录下，数据按照时间保存，可以根据时间来查询相应的性能数据。
你也可以使用sar在命令行下得到一个实时的执行结果，收集的数据可以包括CPU利用率、内存页面、网络I/O等等。下面的命令表示用sar执行5次，间隔时间为3秒：
<a href="http://www.linuxfly.org/attachment/1169008875_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008875_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a>
八、free
free命令显示系统的所有内存的使用情况，包括空闲内存、被使用的内存和交换内存空间。Free命令显示也包括一些内核使用的缓存和缓冲区的信息。
当使用free命令的时候，需要记住linux的内存结构和虚拟内存的管理方法，比如空闲内存数量的限制，还有swap空间的使用并不标志一个内存瓶颈的出现。
<a href="http://www.linuxfly.org/attachment/1169008909_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008909_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a>
Free命令有用的参数：</p>
<p>引用</p>
<p>·-b,-k,-m和-g分别按照bytes, kilobytes, megabytes, gigabytes显示结果。
·-l区别显示low和high内存
·-c {count}显示free输出的次数</p>
<p>九、Pmap
pmap命令显示一个或者多个进程使用内存的数量，你可以用这个工具来确定服务器上哪个进程占用了过多的内存从而导致内存瓶颈。
<a href="http://www.linuxfly.org/attachment/1169008975_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169008975_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a>
十、Strace
strace截取和记录进程的系统调用信息，还包括进程接受的命令信号。这是一个有用的诊断和调试工具，系统管理员可以通过strace来解决程序上的问题。
命令格式，需要指定需要监测的进程ID。这个多为开发人员使用。</p>
<p>strace -p <pid></pid></p>
<p>十一、ulimit
可以通过ulimit来控制系统资源的使用。请看以前的日志：<a href="http://www.linuxfly.org/post/73.htm">使用ulimit和proc去调整系统参数</a>
十二、Mpstat
mpstat命令也是sysstat包的一部分。Mpstat命令用于监测一个多CPU系统中每个可用CPU的情况。Mpstat命令可以显示每个CPU或者所有CPU的运行情况，同时也可以像vmstat命令那样使用参数进行一定频率的采样结果的监测。
<a href="http://www.linuxfly.org/attachment/1169009042_0.jpg"><img title="点击在新窗口中浏览此图片" src="http://www.linuxfly.org/attachment/1169009042_0.jpg" border="0" alt="点击在新窗口中浏览此图片" width="500" /></a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
      <a href="/2009/11/03/zz-linux-performance-command-tools" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/zz-fuser-usage" title="fuser命令（转）" rel="bookmark">fuser命令（转）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>fuser：使用文件或者套节字来表示识别进程。我常用的他的两个功能：查看我需要的进程和我要杀死我查到的进程。</p>
<p>比如当你想umount光驱的时候，结果系统提示你设备正在使用或者正忙，可是你又找不到到底谁使用了他。这个时候fuser可派上用场了。</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># umount /media/cdrom</span>
umount: /media/cdrom: device is busy
umount: /media/cdrom: device is busy
eject: unmount of <span class="sb">`</span>/media/cdrom<span class="s1">&#39; failed</span>
<span class="s1">[root@lancy sbin]# fuser /mnt/cdrom</span>
<span class="s1">/mnt/cdrom: 4561c 5382c</span>
<span class="s1">[root@lancy sbin]# ps -ef |egrep &#39;</span><span class="o">(</span>4561|5382<span class="o">)</span><span class="err">&#39;</span> |grep -v grep
root 4561 4227 0 20:13 pts/1 00:00:00 bash
root 5382 4561 0 21:42 pts/1 00:00:00 vim Autorun.inf
</code></pre></div>
<p>示例中，我想弹出光驱，系统告诉我设备忙着，于是采用fuser命令，参数是你文件或scoket，fuser将查出那些使用了他。</p>
<p>4561c,5382c表示目前用两个进程在占用着/mnt/cdrom，分别是4561,5382,进程ID后的字母表示占用资源的方式，有下面几种表示：</p>
<pre><code>c 当前路径(current directory.)我的理解是表示这个资源的占用是以文件目录方式，也就是进进入了需要释放的资源的路径，这是最常用的资源占用方式。
e 正在运行可执行文件（executable being run.），比如运行了光盘上的某个程序
f 打开文件（ open file），缺省模式下f忽略。所以上面的例子中，虽然是开打了光盘上的Autorun.inf文件，但是给出的标识是c，而不是f。
r root目录（root directory）.没有明白什么意思，难道是说进入了/root这个特定目录？
m mmap文件或者共享库( mmap’ed file or shared library).这应该是说某个进程使用了你要释放的资源的某个共享文件。
</code></pre>
<p>在查找的同时，你还可定指定一些参数，比如</p>
<pre><code>-k 杀死这些正在访问这些文件的进程。除非使用-signal修改信号，否则将发送SIGKILL信号。
-i 交互模式
-l 列出所有已知的信号名称。
-n 空间，选择不同的名字空间，可是file,udp,tcp。默认是file，也就是文件。
-signal 指定发送的信号，而不是缺省的SIGKILL
-4 仅查询IPV4套接字
-6 仅查询IPV6套接字
- 重置所有的选项，将信息设回SIGKILL
</code></pre>
<p>再看下面的例子</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># fuser -l</span>
HUP INT QUIT ILL TRAP ABRT IOT BUS FPE KILL USR1 SEGV USR2 PIPE
ALRM TERM
STKFLT CHLD CONT STOP TSTP TTIN TTOU URG XCPU XFSZ VTALRM PROF
WINCH IO PWR SYS
UNUSED
</code></pre></div>
<p>现在我们试试fuser -k的威力：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># fuser -k /mnt/cdrom</span>
/mnt/cdrom: 4561c 5382c
<span class="nb">kill </span>5382: 没有那个进程
No automatic removal. Please use umount /media/cdrom
<span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># eject</span>
</code></pre></div>
<p>套节字方式的使用：</p>
<div class="highlight"><pre><code class="bash"><span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># fuser -4 -n tcp 3306</span>
here: 3306
3306/tcp: 5595
<span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># ps -ef |grep 5595 |grep -v grep</span>
mysql 5595 5563 0 22:24 pts/0 00:00:00 /usr/libexec/mysqld
--defaults-file<span class="o">=</span>/etc/my.cnf --basedir<span class="o">=</span>/usr --datadir<span class="o">=</span>/var/lib/mysql
--user<span class="o">=</span>mysql --pid-file<span class="o">=</span>/var/run/mysqld/mysqld.pid --skip-locking
--socket<span class="o">=</span>/var/lib/mysql/mysql.sock
<span class="o">[</span>root@lancy sbin<span class="o">]</span><span class="c"># fuser -4 -n tcp 80</span>
here: 80
80/tcp: 5685 5688 5689 5690 5691 5692 5693 5694 5695
</code></pre></div>
      <a href="/2009/11/03/zz-fuser-usage" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/zz-dd-usage" title="dd命令使用详解（转）" rel="bookmark">dd命令使用详解（转）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <ol>
  <li>命令简介</li>
</ol>
<p>dd 的主要选项：
指定数字的地方若以下列字符结尾乘以相应的数字:
b=512, c=1, k=1024, w=2, xm=number m</p>
<p>if=file
输入文件名，缺省为标准输入。
of=file
输出文件名，缺省为标准输出。
ibs=bytes
一次读入 bytes 个字节(即一个块大小为 bytes 个字节)。
obs=bytes
一次写 bytes 个字节(即一个块大小为 bytes 个字节)。
bs=bytes
同时设置读写块的大小为 bytes ，可代替 ibs 和 obs 。
cbs=bytes
一次转换 bytes 个字节，即转换缓冲区大小。
skip=blocks
从输入文件开头跳过 blocks 个块后再开始复制。
seek=blocks
从输出文件开头跳过 blocks 个块后再开始复制。(通常只有当输出文件是磁盘或磁带时才有效)。
count=blocks
仅拷贝 blocks 个块，块大小等于 ibs 指定的字节数。
conv=conversion[,conversion&hellip;]
用指定的参数转换文件。</p>
<p>转换参数:
ascii 转换 EBCDIC 为 ASCII。
ebcdic 转换 ASCII 为 EBCDIC。
ibm 转换 ASCII 为 alternate EBCDIC.
block 把每一行转换为长度为 cbs 的记录，不足部分用空格填充。
unblock 使每一行的长度都为 cbs ，不足部分用空格填充。
lcase 把大写字符转换为小写字符。
ucase 把小写字符转换为大写字符。
swab 交换输入的每对字节。
noerror 出错时不停止。
notrunc 不截短输出文件。
sync 把每个输入块填充到ibs个字节，不足部分用空(NUL)字符补齐。</p>
<p>2.实例分析</p>
<p>2.1.数据备份与恢复</p>
<p>2.1.1整盘数据备份与恢复
备份：
dd if=/dev/hdx of=/dev/hdy
将本地的/dev/hdx整盘备份到/dev/hdy
dd if=/dev/hdx of=/path/to/image
将/dev/hdx全盘数据备份到指定路径的image文件
dd if=/dev/hdx | gzip &gt;/path/to/image.gz
备份/dev/hdx全盘数据，并利用gzip工具进行压缩，保存到指定路径
恢复：
dd if=/path/to/image of=/dev/hdx
将备份文件恢复到指定盘
gzip -dc /path/to/image.gz | dd of=/dev/hdx
将压缩的备份文件恢复到指定盘</p>
<p>2.1.2.利用netcat远程备份
dd if=/dev/hda bs=16065b | netcat 1234
在源主机上执行此命令备份/dev/hda
netcat -l -p 1234 | dd of=/dev/hdc bs=16065b
在目的主机上执行此命令来接收数据并写入/dev/hdc
netcat -l -p 1234 | bzip2 &gt; partition.img
netcat -l -p 1234 | gzip &gt; partition.img
以上两条指令是目的主机指令的变化分别采用bzip2 gzip对数据进行压缩，并将备份文件保存在当前目录。</p>
<p>2.1.3.备份MBR
备份：
dd if=/dev/hdx of=/path/to/image count=1
bs=512
备份磁盘开始的512Byte大小的MBR信息到指定文件
恢复：
dd if=/path/to/image of=/dev/hdx
将备份的MBR信息写到磁盘开始部分</p>
<p>2.1.4.备份软盘
dd if=/dev/fd0 of=disk.img count=1
bs=1440k
将软驱数据备份到当前目录的disk.img文件</p>
<p>2.1.5.拷贝内存资料到硬盘
dd if=/dev/mem of=/root/mem.bin
bs=1024
将内存里的数据拷贝到root目录下的mem.bin文件</p>
<p>2.1.6.从光盘拷贝iso镜像
dd if=/dev/cdrom of=/root/cd.iso
拷贝光盘数据到root文件夹下，并保存为cd.iso文件</p>
<p>2.2.增加Swap分区文件大小
dd if=/dev/zero of=/swapfile bs=1024 count=262144
创建一个足够大的文件（此处为256M）
mkswap /swapfile
把这个文件变成swap文件
swapon /swapfile
启用这个swap文件
/swapfile swap swap defaults 0 0
在每次开机的时候自动加载swap文件, 需要在 /etc/fstab文件中增加一行</p>
<p>2.3.销毁磁盘数据
dd if=/dev/urandom of=/dev/hda1
利用随机的数据填充硬盘，在某些必要的场合可以用来销毁数据。执行此操作以后，/dev/hda1将无法挂载，创建和拷贝操作无法执行。</p>
<p>2.4.磁盘管理</p>
<p>2.4.1.得到最恰当的block size
dd if=/dev/zero bs=1024 count=1000000
of=/root/1Gb.file
dd if=/dev/zero bs=2048 count=500000
of=/root/1Gb.file
dd if=/dev/zero bs=4096 count=250000
of=/root/1Gb.file
dd if=/dev/zero bs=8192 count=125000
of=/root/1Gb.file
通过比较dd指令输出中所显示的命令执行时间，即可确定系统最佳的blocksize大小</p>
<p>2.4.2测试硬盘读写速度
dd if=/root/1Gb.file bs=64k | dd
of=/dev/null
dd if=/dev/zero of=/root/1Gb.file bs=1024
count=1000000
通过上两个命令输出的执行时间，可以计算出测试硬盘的读／写速度</p>
<p>2.4.3.修复硬盘
dd if=/dev/sda of=/dev/sda
当硬盘较长时间（比如1，2年）放置不使用后，磁盘上会产生magnetic fluxpoint。当磁头读到这些区域时会遇到困难，并可能导致I/O错误。当这种情况影响到硬盘的第一个扇区时，可能导致硬盘报废。上边的命令有可能使这些数据起死回生。且这个过程是安全，高效的</p>
      <a href="/2009/11/03/zz-dd-usage" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/zz-curl-usage" title="curl使用简单介绍（转）" rel="bookmark">curl使用简单介绍（转）</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>原文地址： <a href="http://www.linuxidc.com/Linux/2008-01/10891p2.htm">http://www.linuxidc.com/Linux/2008-01/10891p2.htm</a>
Curl是Linux下一个很强大的http命令行工具，其功能十分强大。</p>
<ol>
  <li>
    <p>二话不说，先从这里开始吧！
$ curl http://www.linuxidc.com
回车之后，www.linuxidc.com的html就稀里哗啦地显示在屏幕上了~</p>
  </li>
  <li>
    <p>嗯，要想把读过来页面存下来，是不是要这样呢？
$ curl http://www.linuxidc.com &gt; page.html
当然可以，但不用这么麻烦的！
用curl的内置option就好，存下http的结果，用这个option: -o
$ curl -o page.html http://www.linuxidc.com
这样，你就可以看到屏幕上出现一个下载页面进度指示。等进展到100%，自然就 OK咯</p>
  </li>
  <li>
    <p>什么什么？！访问不到？肯定是你的proxy没有设定了。
使用curl的时候，用这个option可以指定http访问所使用的proxy服务器及其端口： -x
$ curl -x 123.45.67.89:1080 -o page.html http://www.linuxidc.com</p>
  </li>
  <li>
    <p>访问有些网站的时候比较讨厌，他使用cookie来记录session信息。
像IE/NN这样的浏览器，当然可以轻易处理cookie信息，但我们的curl呢？&hellip;..
我们来学习这个option: -D
这个是把http的response里面的cookie信息存到一个特别的文件中去
$ curl -x 123.45.67.89:1080 -o page.html -D cookie0001.txt http://www.linuxidc.com
这样，当页面被存到page.html的同时，cookie信息也被存到了cookie0001.txt里面了</p>
  </li>
</ol>
<p>5）那么，下一次访问的时候，如何继续使用上次留下的cookie信息呢？要知道，很多网站都是靠监视你的cookie信息，来判断你是不是不按规矩访问他们的网站的。
这次我们使用这个option来把上次的cookie信息追加到http request里面去： -b
$ curl -x 123.45.67.89:1080 -o page1.html -D cookie0002.txt -b cookie0001.txt http://www.linuxidc.com
这样，我们就可以几乎模拟所有的IE操作，去访问网页了！</p>
<p>6）稍微等等
~我好像忘记什么了
~
对了！是浏览器信息
有些讨厌的网站总要我们使用某些特定的浏览器去访问他们，有时候更过分的是，还要使用某些特定的版本
NND，哪里有时间为了它去找这些怪异的浏览器呢！？
好在curl给我们提供了一个有用的option，可以让我们随意指定自己这次访问所宣称的自己的浏览器信息： -A
$ curl -A &ldquo;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)&rdquo; -x 123.45.67.89:1080 -o page.html -D cookie0001.txt http://www.linuxidc.com
这样，服务器端接到访问的要求，会认为你是一个运行在Windows 2000上的IE6.0，嘿嘿嘿，其实也许你用的是苹果机呢！
而&rdquo;Mozilla/4.73 [en] (X11; U; Linux 2.2; 15 i686&rdquo;则可以告诉对方你是一台PC上跑着的Linux，用的是Netscape 4.73，呵呵呵</p>
<p>7） 另外一个服务器端常用的限制方法，就是检查http访问的referer。比如你先访问首页，再访问里面所指定的下载页，这第二次访问的referer地址就是第一次访问成功后的页面地址。这样，服务器端只要发现对下载页面某次访问的referer地址不是首页的地址，就可以断定那是个盗连了~讨厌讨厌~我就是要盗链~！！
幸好curl给我们提供了设定referer的option： -e
$ curl -A &ldquo;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)&rdquo; -x 123.45.67.89:1080 -e &ldquo;mail.linuxidc.com&rdquo; -o page.html -D cookie0001.txt http://www.linuxidc.com
这样，就可以骗对方的服务器，你是从mail.linuxidc.com点击某个链接过来的了，呵呵呵</p>
<p>8）写着写着发现漏掉什么重要的东西了！——- 利用curl 下载文件
刚才讲过了，下载页面到一个文件里，可以使用 -o ，下载文件也是一样。比如，
$ curl -o 1.jpg http://cgi2.tky.3web.ne.jp/~zzh/screen1.JPG
这里教大家一个新的option： -O 大写的O，这么用：
$ curl -O http://cgi2.tky.3web.ne.jp/~zzh/screen1.JPG
这样，就可以按照服务器上的文件名，自动存在本地了！
再来一个更好用的。
如果screen1.JPG以外还有screen2.JPG、screen3.JPG、&hellip;.、screen10.JPG需要下载，难不成还要让我们写一个script来完成这些操作？
不干！
在curl里面，这么写就可以了：
$ curl -O http://cgi2.tky.3web.ne.jp/~zzh/screen[1-10].JPG
呵呵呵，厉害吧？！ ~</p>
<p>9）再来，我们继续讲解下载！
$ curl -O <a href="http://cgi2.tky.3web.ne.jp/~{zzh,nick}/[001-201].JPG">http://cgi2.tky.3web.ne.jp/~{zzh,nick}/[001-201].JPG</a>
这样产生的下载，就是
~zzh/001.JPG
~zzh/002.JPG
&hellip;
~zzh/201.JPG
~nick/001.JPG
~nick/002.JPG
&hellip;
~nick/201.JPG
够方便的了吧？哈哈哈
咦？高兴得太早了。
由于zzh/nick下的文件名都是001，002&hellip;，201，下载下来的文件重名，后面的把前面的文件都给覆盖掉了 ~
没关系，我们还有更狠的！
$ curl -o #2_#1.jpg http://cgi2.tky.3web.ne.jp/~{zzh,nick}/[001-201].JPG
—这是&hellip;..自定义文件名的下载？ 对头，呵呵！
这样，自定义出来下载下来的文件名，就变成了这样：原来： ~zzh/001.JPG —-&gt; 下载后：001-zzh.JPG 原来： ~nick/001.JPG —-&gt; 下载后：001-nick.JPG
这样一来，就不怕文件重名啦，呵呵</p>
<p>9）继续讲下载
我们平时在windows平台上，flashget这样的工具可以帮我们分块并行下载，还可以断线续传。curl在这些方面也不输给谁，嘿嘿比如我们下载screen1.JPG中，突然掉线了，我们就可以这样开始续传
$ curl -c -O http://cgi2.tky.3wb.ne.jp/~zzh/screen1.JPG
当然，你不要拿个flashget下载了一半的文件来糊弄我。别的下载软件的半截文件可不一定能用哦 ~</p>
<p>分块下载，我们使用这个option就可以了： -r
举例说明
比如我们有一个http://cgi2.tky.3web.ne.jp/~zzh/zhao1.MP3 要下载（赵老师的电话朗诵 :D）我们就可以用这样的命令：
$ curl -r 0-10240 -o &ldquo;zhao.part1&rdquo; http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.MP3 &amp;
$ curl -r 10241-20480 -o &ldquo;zhao.part1&rdquo; http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.MP3 &amp;
$ curl -r 20481-40960 -o &ldquo;zhao.part1&rdquo; http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.MP3 &amp;
$ curl -r 40961- -o &ldquo;zhao.part1&rdquo; http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.MP3
这样就可以分块下载啦。不过你需要自己把这些破碎的文件合并起来如果你用UNIX或苹果，用 cat zhao.part* &gt; zhao.MP3就可以如果用的是Windows，用copy /b 来解决吧，呵呵
上面讲的都是http协议的下载，其实ftp也一样可以用。用法嘛，
$ curl -u name:passwd ftp://ip:port/path/file
或者大家熟悉的
$ curl ftp://name:passwd@ip:port/path/file</p>
<ol>
  <li>说完了下载，接下来自然该讲上传咯上传的option是 -T
比如我们向ftp传一个文件：
$ curl -T localfile -u name:passwd ftp://upload_site:port/path/
当然，向http服务器上传文件也可以比如
$ curl -T localfile http://cgi2.tky.3web.ne.jp/~zzh/abc.cgi
注意，这时候，使用的协议是HTTP的PUT method</li>
</ol>
<p>刚才说到PUT，嘿嘿，自然让老服想起来了其他几种methos还没讲呢！ GET和POST都不能忘哦。
http提交一个表单，比较常用的是POST模式和GET模式
GET模式什么option都不用，只需要把变量写在url里面就可以了比如：
$ curl http://www.linuxidc.com/login.cgi?user=nickwolfe&amp;password=12345</p>
<p>而POST模式的option则是 -d
比如:
$ curl -d &ldquo;user=nickwolfe&amp;password=12345&rdquo; http://www.linuxidc.com/login.cgi
就相当于向这个站点发出一次登陆申请~
到底该用GET模式还是POST模式，要看对面服务器的程序设定。</p>
<p>一点需要注意的是，POST模式下的文件上的文件上传，比如</p>
<div class="highlight"><pre><code class="html"><span class="nt">&lt;form</span> <span class="na">method=</span><span class="s">&quot;POST&quot;</span> <span class="na">enctype=</span><span class="s">&quot;multipar/form-data&quot;</span> <span class="na">action=</span><span class="s">&quot;http://cgi2.tky.3web.ne.jp/~zzh/up_file.cgi&quot;</span><span class="nt">&gt;</span>
<span class="nt">&lt;input</span> <span class="na">type=</span><span class="s">submit</span> <span class="na">name=</span><span class="s">nick</span> <span class="na">value=</span><span class="s">&quot;go&quot;</span><span class="nt">&gt;</span>
</code></pre></div>
<p>这样一个HTTP表单，我们要用curl进行模拟，就该是这样的语法：
$ curl -F upload=@localfile -F nick=go http://cgi2.tky.3web.ne.jp/~zzh/up_file.cgi
罗罗嗦嗦讲了这么多，其实curl还有很多很多技巧和用法比如 https的时候使用本地证书，就可以这样
$ curl -E localcert.pem https://remote_server
再比如，你还可以用curl通过dict协议去查字典~
$ curl dict://dict.org/d:computer</p>
<ol>
  <li>
    <p>开启gzip请求
curl -I http://www.sina.com.cn/ -H &ldquo;Accept-Encoding:gzip,defalte&rdquo;</p>
  </li>
  <li>
    <p>监控网页的响应时间
curl -o /dev/null -s -w &ldquo;time_connect:%{time_connect}\ntime_starttransfer:%{time_starttransfer}\ntime_total:%{time_total}\n&rdquo; http://www.kklinux.com</p>
  </li>
  <li>
    <p>监控站点可用性
curl -o /dev/null -s -w %{http_code} http://www.kklinux.com</p>
  </li>
</ol>
      <a href="/2009/11/03/zz-curl-usage" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/using-shell-variable-in-awk-script" title="awk调用shell变量" rel="bookmark">awk调用shell变量</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>今天的问题：因为某个原因，需要长期探测对某机器的ping值情况。期望的输出格式是“丢包率 响应时间均值”。</p>
<p>写个小脚本，最后echo一下，自然好办的很。不过在crontab里看到之前大都有一条任务写的是ping 1.2.3.4，于是想：能不能让这个脚本的内容也尽量写在一句话里呢？</p>
<p>连动命令的话，输出结果都分了行。于是开始摸索awk的内外变量调用问题。</p>
<p>网上说明很多，大都是BEGIN或者-v的办法。一一试过后，发现其结果也都是分行显示的。</p>
<p>最后大海淘沙般找出了适用的写法。结果全在&rsquo;&ldquo;`的区分上——而且我至今不知道为啥非得按如下写法才行：</p>
<div class="highlight"><pre><code class="bash"><span class="nv">ls</span><span class="o">=</span><span class="sb">`</span>ping -c 5 1.2.3.4 | grep loss | awk -F, <span class="s1">&#39;{print $3}&#39;</span><span class="sb">`</span>;ping -c 5 1.2.3.4 | grep rtt | awk -F/ <span class="s1">&#39;{print &quot;&#39;</span><span class="s2">&quot;$ls&quot;</span><span class="s1">&#39; avg &quot; $5 &quot;ms&quot;}&#39;</span>
</code></pre></div>
<p>执行显示结果如下：
0% packet loss avg 17.486ms
——————————————————————————————
时隔近月，在熟悉了awk的变量以后，我发现其实没有这么复杂，只要下面这样一句就简单搞定了：</p>
<div class="highlight"><pre><code class="bash">ping -c 5 1.2.3.4|awk <span class="s1">&#39;BEGIN{RS=&quot;##&quot;;FS=&quot;,|/&quot;}{print $3,$5,$8“ms”}&#39;</span>
</code></pre></div>
<p>执行结果同上。</p>
      <a href="/2009/11/03/using-shell-variable-in-awk-script" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/test-squid-by-http_load" title="squid压力测试" rel="bookmark">squid压力测试</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#testing-ref" title="testing" rel="category tag">testing</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>向公司申请了台设备做测试机，打算把公司各种应用服务都自己练练，熟悉一下。先从最传统的squid开始做压力测试。先发一个小东东http_load的测试：</p>
<ul>
  <li>实验环境：</li>
</ul>
<p>服务器硬件条件：内存2096M，CPU2.33GHz，硬盘70G；
Squid版本：Version 2.6.STABLE22</p>
<ul>
  <li>实验架构：</li>
</ul>
<p>单台服务器：沈阳
web页面由nginx发布，采用基础配置，监听8080端口，网站页面类型包括.htm/html/css/js/xml；
前端由squid代理，采用公司默认配置，由cache_peer请求本机页面，监听80端口；
测试访问地点：北京</p>
<ul>
  <li>测试方法：</li>
</ul>
<p>创建url文件，添加url记录共11条，其中html六条，js三条，css/xml各一条。（本来还有一个tar.gz文件，结果文件有近2M，影响测试结果，放弃）
根据网友经验，采取fetches参数配合parallel参数测试。</p>
<p>命令如下：./http_load -f 3000 -p 100 url &gt; result.txt</p>
<p>根据情况调整parallel参数。</p>
<ul>
  <li>测试结果：</li>
</ul>
<p>** 当p到180以上后，测试过程中就开始出现类似下面这样提示：</p>
<pre><code>http://www.test.com/ts.js: Connection timed out
http://www.test.com/ts.js: byte count wrong
</code></pre>
<p>而结果中的fetches/sec则在650-850之间随机出现，属于不太稳定的状态了。而p再往上提高（我从200一直实验到500）， fetches/sec基本都在800左右，而msecs/connect则从50上升到255左右。
** 当p在140以下时，fetches/sec在1500以上，msecs/connect在0.2左右，bytes/sec在85000左右（此时服务器iptraf -d eth1查看流量大概每秒12M）；
** 当p到140以上后，f/s迅速下降到1000左右，ms/c则上升到10以上；parallel从150到180的过程中，f/s、ms/c和b/s基本是均速变化的；同时查看服务器的iostat或者vmstat，一般us在90%多，bo偶然有1出现（后来发现是因为我放的测试文件大小相差较大）。</p>
<ul>
  <li>实验结论</li>
</ul>
<p>squid服务在并发数140以下时，能够提供优质的服务，140以上，性能逐渐下降，当并发数到200以上后，遭遇瓶颈，主要在CPU方面。</p>
<ul>
  <li>实验疑问</li>
</ul>
<p>公司在线服务的设备，一般eth1流量都能跑到60M，有些甚至上了100M，这跟测试结果相差也太大了？</p>
<ul>
  <li>参考资料
<a href="http://www.hiadmin.com/tomcat-%e5%b9%b6%e5%8f%91%e6%b5%8b%e8%af%95/">http://www.hiadmin.com/tomcat-%e5%b9%b6%e5%8f%91%e6%b5%8b%e8%af%95/</a>
./http_load -parallel 200 -seconds 10 urls
  按照固定时间来结束测试,这样可以比较相同时间内被测服务器的响应速度.
  ./http_load -parallel 200 -fetches 1000 urls
  按照固定申请数来测试,这样可以比较相同访问量下返回的响应速度.
  虽然两者都可以获取到服务器的响应速度
  但是使用fetches更容易让被测服务器收到压力
  由于seconds控制测试时间,很有可能在短时间内测试客户端并没有发起足够数量的请求
  而服务端在收到足够压力之前,测试就已经结束了.
  有一些情况,诸如内存泄漏以及资源回收不利或者对后面的响应速度越来越慢等情况
  在这种测试条件下不容易发生
  而使用fetchs,能够让客户端保证确定请求数的全部处理.
  使用时间作为控制参数
  会由于测试人员不够耐心而人为将seconds参数设置过小
  导致测试结果失去意义
  所以,最后建议使用fetches作为测试参数.用以作为基准进行比较
  如果httpd_load获取到的页面数据和上次不一致
  则会报错byte count wrong
  如果是动态页面,由于返回数据内容不同.则此报错可以忽略</li>
</ul>
      <a href="/2009/11/03/test-squid-by-http_load" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/tcpwrapper" title="tcpwrapper" rel="bookmark">tcpwrapper</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#linux-ref" title="linux" rel="category tag">linux</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>今天去新浪面试。有一道笔试题，考的是tcpwrapper的用法。因为没见过这个东东，所以百度一下，选几篇文章总结一下。</p>
<p>首先，tcpwrapper是unix上的工具，1990年就诞生了。至于它和iptables的不同，看到有人说是TCP/IP层的不同。说iptables是网络层的，tcpwrapper是应用层的。对不对，且看tcpwrapper的使用先：</p>
<ol>
  <li>
    <p>部署：首选当然是用安装包，要是编译源码，参见如下文：http://echo.sharera.com/blog/BlogTopic/9379.htm，虽然作者说自己笨笨的乱写，那也比我强多了</p>
  </li>
  <li>
    <p>开启日志：在/etc/syslog.conf里添加如下字段即可</p>
  </li>
</ol>
<p>tcpwrapper loglocal3.info /var/log/tcplog</p>
<p>这个时候要记得重启日志服务。可以使用kill -HUP syslogd进程号的方法（nnd，这也是今天的笔试题之一）。</p>
<ol>
  <li>配置文件：/etc/hosts.allow</li>
</ol>
<p>（本来还有个hosts.deny的）
编写规则是“servicename:hostname[:shellcmd]”</p>
<p>tcpwrapper监控的是inetd里的启动服务，用telnet举例如下</p>
<div class="highlight"><pre><code class="bash">telnet:ALL
EXCEPT LOCAL, .M-gtuiw.com
<span class="nb">echo</span> <span class="s2">&quot;request from %d@%h:&quot;</span> &gt;&gt; /var/log/telnet.log;
<span class="k">if</span> <span class="o">[</span> %h !<span class="o">=</span> <span class="s2">&quot;OS.M-gtuiw.com:&quot;</span> <span class="o">]</span> ; <span class="k">then</span>
<span class="k">    </span>finge -l @%h &gt;&gt; /var/log/telnet.log
<span class="k">fi</span>
</code></pre></div>
<p>意即允许除了本机和M-gtuiw.com域下主机以外的所有telnet请求，并以“请求来自服务名@主机名”的方式记录进日志。（注意：EXCEPT也可以用在servicename后面）</p>
<p>和iptables一样（好像说反了，其实应该是iptables和tcpd一样），这个allow和deny的规则也是讲究先来后到的，所以会有个ALL:ALL:deny收尾（如果单有deny文件，就在里头写ALL:ALL就可以了）。</p>
<ol>
  <li>调试</li>
</ol>
<p>tcpdchk -v可以看到tcpd的全部规则设置和错误提示
tcpdmatch servicename hostname可以具体查询某条规则</p>
<ol>
  <li>inetd服务配置</li>
</ol>
<p>相关的有两个文件，一个是/etc/services，这里定义了各种服务使用的协议和占用的端口（本文中出现的第三个新浪笔试考题了哦~~）；一个是/etc/inetd.conf，这里定义了各项服务的类型、协议、监听方式、用户、程序、参数——如果启用tcpwrapper的话，程序就都是/usr/sbin/tcpd了。比如telnet的配置行如下：</p>
<p>telnet    stream    tcp    nowait    root    /usr/sbin/tcpd    in.telnetd</p>
<ol>
  <li>日志结果，直接摘抄一段如下：</li>
</ol>
<div class="highlight"><pre><code class="bash">Jul 31 22:00:52 <span class="o">[</span>url<span class="o">]</span>www.test.org<span class="o">[</span>/url<span class="o">]</span> in.telnetd<span class="o">[</span>4365<span class="o">]</span>: connect from 10.68.32.1
Jul 31 22:02:10 <span class="o">[</span>url<span class="o">]</span>www.test.org<span class="o">[</span>/url<span class="o">]</span> in.telnetd<span class="o">[</span>4389<span class="o">]</span>: connect from 10.68.32.5
Jul 31 22:04:58 <span class="o">[</span>url<span class="o">]</span>www.test.org<span class="o">[</span>/url<span class="o">]</span> in.ftpd<span class="o">[</span>4429<span class="o">]</span>: connect from 10.68.32.3
</code></pre></div>
<p>以上说了这么多，都是unix上的，最后来一句，在linux上，xinetd就是这个inetd+tcpwrapper了。何况还有强大的iptables……它可不像tcpwrapper只能管tcp协议的服务哦~~</p>
<p>参考文章：</p>
<p><a href="http://jianjian.blog.51cto.com/35031/41949">http://jianjian.blog.51cto.com/35031/41949</a>
<a href="http://echo.sharera.com/blog/BlogTopic/9379.htm">http://echo.sharera.com/blog/BlogTopic/9379.htm</a>
<a href="http://blog.chinaunix.net/u/26264/showart_971334.html">http://blog.chinaunix.net/u/26264/showart_971334.html</a>
http://www.linuxdiyf.com/viewarticle.php?id=18335</p>
      <a href="/2009/11/03/tcpwrapper" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/some-useful-shell-techniques" title="shell处理技巧" rel="bookmark">shell处理技巧</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#bash-ref" title="bash" rel="category tag">bash</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>脚本经常用的上，但老是记不住的一些东西：</p>
<p>一、逻辑运算</p>
<pre><code>eq 相等
ne、neq 不相等
gt 大于
lt 小于
gte、ge 大于等于
lte、le 小于等于
not 非
mod 求模
is [not] div by 是否能被某数整除
is [not] even 是否为偶数
is [not] odd 是否为奇数
-a  存在则为真
-b  存在且是一个块特殊文件则为真
-c  存在且是一个字特殊文件则为真
-d  存在且是一个目录则为真
-e  存在则为真
-f  存在且是一个普通文件则为真
-g  存在且已经设置了SGID则为真
-h  存在且是一个符号连接则为真
-k  存在且已经设置了粘制位则为真
-p  存在且是一个名字管道(F如果O)则为真
-r  存在且是可读的则为真
-s  存在且大小不为0则为真
-t  打开且指向一个终端则为真
-u  存在且设置了SUID则为真
-w  存在且是可写的则为真
-x  存在且是可执行的则为真
-O  存在且属有效用户ID则为真
-G  存在且属有效用户组则为真
-L  存在且是一个符号连接则为真
-N  存在 and has been mod如果ied since it was last
read则为真
-S  存在且是一个套接字则为真
</code></pre>
<p>二、特殊变量</p>
<pre><code>$# 传递到脚本的参数个数
$* 以一个单字符串显示所有向脚本传递的参数。与位置变量不同，此选项参数可超过9个
$$ 脚本运行的当前进程ID号
$! 后台运行的最后一个进程的进程ID号
$@ 与$#相同，但是使用时加引号，并在引号中返回每个参数
$- 显示shell使用的当前选项，与set命令功能相同
$? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误
$((...)) 对括号内的表达式求值
</code></pre>
<p>三、打印格式</p>
<table>
  <tbody>
    <tr>
      <td>free -m</td>
      <td>awk &lsquo;/Mem/{printf &ldquo;%.0fn&rdquo;,$1*0.9}&rsquo;</td>
    </tr>
  </tbody>
</table>
<p>自动取整数: ％代表任意长度，0表示保留.后0位数字</p>
<p>四、翻转文件</p>
<p>sed &lsquo;1!G;h;$!d&rsquo; file
sed -n &lsquo;1!G;h;$p&rsquo; file</p>
<p>sed的man说法如下：
    h
    H        Copy/append
    pattern space to hold space.
    g
    G        Copy/append
    hold space to pattern space.
这两个space，我的理解就是hold是厂房（原料和成品都放这，汗～），pattern是流水线。  <br />
如果是正常的sed，应该是所有的原料一起上流水线处理，然后统一成品输出。所以顺序不变；但上面这个命令，每处理一行就清空一次流水线，最后成品就成倒序了。
还有一个x，用来互换两个space的文本。</p>
<p>五、正则表达式</p>
<pre><code>pattern{n}：只用来匹配前面pattern出现的次数.n为次数。如a{2}匹配aa.
pattern{n,}：含义同上，但次数最少为n.如a{2,}匹配aa,aaa,aaaa,.....
pattern{n,m}：含义同上，但次数在n和m之间。如a{2,4}匹配aa,aaa,aaaa三个
[0123456789]或[0-9] ：假定要匹配任意一个数字
[a-z] ：任意小写字母
[A-Za-z] ：任意大小写字母
[S,s] ：匹配大小写S
[0-9]{3}.[0-9]{3}.[0-9]{3}.[0-9]{3} ：匹配IP地址 [0-9]{3}三个0-9组成的字符串；. ：匹配点（注意这里点是特殊的字符，所以要用""来屏蔽其含义）
用于grep和awk的类名（其他能不能我还不知道）
[[:upper:]]   表示[A-Z]
[[:alnum:]]   表示[0-9a-zA-Z]
[[:lower:]]   表示[a-z]
[[:space:]]   表示空格或者tab键
[[:digit:]]   表示[0-9]
[[:alpha:]]   表示[a-zA-Z]
</code></pre>
      <a href="/2009/11/03/some-useful-shell-techniques" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/priority-of-squid-domain-resolve" title="squid问题-域名解析" rel="bookmark">squid问题-域名解析</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#squid-ref" title="squid" rel="category tag">squid</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>今天有老客户下单修改源IP地址1.2.3.4为1.2.3.7，一切正常操作过后进行测试，其中有台机器就是狂报404。
在用/home/squid/bin/squidclient -p 80 -m PURGE http://测试url
命令清除缓存，甚至重启dns/squid服务后，其测试访问的first-to-parent地址还是1.2.3.4！！</p>
<p>用dig检查确认内部DNS配置已经生效后，又检查了hosts文件也没有问题。</p>
<p>最后发现是squid.conf里的泛域名配置问题。</p>
<p>这批服务器在升级squid前，曾经在一台机器上测试新版本配置，之后一直没有更改，其中有如下字段：</p>
<div class="highlight"><pre><code class="squid"><span class="k">cache_peer</span><span class="w"> </span><span class="mf">1.2.3.4</span><span class="w"> </span><span class="no">parent</span><span class="w"> </span><span class="m">80</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="no">no-query</span><span class="w"> </span>no-netdb-exchange<span class="w"> </span>originserver<span class="w"></span>
cache_peer_domain<span class="w"> </span><span class="mf">1.2.3.4</span><span class="w"> </span>www.test.com<span class="w"></span>
</code></pre></div>
<p>所以在系统上怎么修改，都没法成功了。
由此可知，CDN加速对域名的解析，是squid配置文件最优先，然后才是系统的hosts文件，最后是DNS服务器。</p>
      <a href="/2009/11/03/priority-of-squid-domain-resolve" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div class="full" style='background-color: #FFF;'>
    <h1 class="entry-title">
      <a href="/2009/11/03/nid-00135-of-dbname" title="修改dbname常见的一个错误NID-00135及解决…" rel="bookmark">修改dbname常见的一个错误NID-00135及解决…</a>
    </h1>
    <p class="alt-font tight">
      <span class="date full-date">
        <abbr class="published" title="2009-11-03 00:00:00 +0800">03 Nov 2009</abbr>
      </span>
      <span>
      </span>
      Posted in&nbsp;
      <a href="/categories.html#database-ref" title="database" rel="category tag">database</a>
    </p>
    <div class="entry-content full-content" style='background-color: #FFF; margin-left: 24px; padding: 10px;'>
      <p>oracle自带有nid用以修改dbname。查看其命令语法如下表所示：</p>
<pre><code>DBNEWID: Release 10.2.0.4.0 - Production on Wed Jun 24 20:06:08 2009
Copyright (c) 1982, 2007,
Oracle.    All
rights reserved.
Keyword
Description                                        (Default)
----------------------------------------------------
TARGET            Username/Password                            (NONE)
DBNAME            New
database name                            (NONE)
LOGFILE
Output Log
(NONE)
REVERT            Revert
failed
change
NO
SETNAME
Set a new database name
only
NO
APPEND            Append
to output log
NO
HELP                Displays
these
messages                NO 其动作描述用“=”表示。
</code></pre>
<p>假设某oracle数据库sys密码为123456，欲更名dbname为aaa，则其修改dbname的命令应如下行所示：</p>
<p>nid target=sys/123456 dbname=aaa</p>
<p>网上关于修改dbname的博客文章和论坛问答，基本都是在windows平台上的操作。其提示要点，在于运行nid系统命令之前，必须将数据库置于mount状态下。以此类推，在linux下的操作步骤，应该如下：</p>
<div class="highlight"><pre><code class="sql"><span class="k">sql</span> <span class="o">&gt;</span> <span class="n">shutdown</span>
<span class="k">immediate</span><span class="p">;</span>
<span class="k">sql</span> <span class="o">&gt;</span> <span class="n">startup</span> <span class="n">mount</span><span class="p">;</span>
<span class="k">sql</span> <span class="o">&gt;</span> <span class="k">host</span> <span class="n">nid</span> <span class="n">target</span><span class="o">=</span><span class="n">sys</span><span class="o">/</span><span class="mi">123456</span> <span class="n">dbname</span><span class="o">=</span><span class="n">aaa</span>
</code></pre></div>
<p>但我按此步骤进行之后，却提示如下字段：
    NID-00135: There are 1 active threads
    Change of database name failed during validation - database is
    intact.
    DBNEWID - Completed with validation errors.
经过多机试验，发现这个错误并非偶然出现一两次而已，至于windows平台下，为何无人提起，就有待日后研究了。
关于这个错误，关键是检查两个地方。
第一是表空间与数据文件的状态：
    SQL&gt; select
    file#,status,name from
    v$datafile;
    FILE#
    STATUS    NAME
    &mdash;&mdash;&mdash;- &mdash;&mdash;-
    &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;
    1
    SYSTEM    /u01/app/oracle/oradata/db1/system01.dbf
    2
    ONLINE    /u01/app/oracle/oradata/db1/undotbs01.dbf
    3
    ONLINE    /u01/app/oracle/oradata/db1/sysaux01.dbf
    4
    ONLINE    /u01/app/oracle/oradata/db1/usertbs.dbf
    5
    ONLINE    /u01/app/oracle/oradata/db1/raocl.dbf
正常情况下，其状态应该是online或者offline。但如果因为历史操作的原因，导致某数据文件的状态变成了recovery，那么就会出问题了。
解决方法也简单，drop掉出错的数据文件就行了。
第二是归档文件的设置：
    SQL&gt; archive log
    list
    Database log
    mode                            Archive
    Mode
    Automatic
    archival
    Enabled
    Archive
    destination                        /u01/app/oracle/product/10.2.0/db1/dbs/arch
    Oldest online log
    sequence
    31
    Next log sequence to
    archive
    33
    Current log
    sequence
    33
    SQL&gt; host ls $ORACLE_HOME/dbs
    alert_db1.log
    arch1<em>29</em>689269707.dbf    control01.ctl
    initdw.ora    spfiledb1.ora.bak
    arch1<em>25</em>689269707.dbf    arch1<em>30</em>689269707.dbf    control02.ctl
    init.ora
    arch1<em>26</em>689269707.dbf    arch1<em>31</em>689269707.dbf    db1<em>ora</em>4704.trc    lkAAA
    arch1<em>27</em>689269707.dbf    arch1<em>32</em>689269707.dbf    hc_db1.dat                lkDB1
    arch1<em>28</em>689269707.dbf    cntrldb1.dbf                        initdb1.ora
    orapwdb1
如果没有设置归档文件路径或者没有归档文件存在，nid也会出错。
设置归档文件模式、路径并手工归档的命令分别如下：
    SQL&gt; alter database archivelog;
    SQL&gt; alter system
    set log_archive_dest_1=&rsquo;location=/u01/app/oracle/oradata/db1/arch&rsquo;;
    SQL&gt; alter system
    archive log current;
注意：归档文件模式也要在mount下设置。
确认完成这两步以后，在重新运行nid系统命令，出现如下字段，即可成功更改dbname了。
    Control Files in database:
    /u01/app/oracle/product/10.2.0/db1/dbs/control01.ctl
    /u01/app/oracle/product/10.2.0/db1/dbs/control02.ctl
    Change database ID and database
    name DB1 to AAA? (Y/[N]) =&gt; Y
    Proceeding with operation
    Changing database ID from 1283133323 to
    1845742016
    Changing database name from DB1
    to AAA
    Control
    File
    /u01/app/oracle/product/10.2.0/db1/dbs/control01.ctl -
    modified
    Control
    File
    /u01/app/oracle/product/10.2.0/db1/dbs/control02.ctl -
    modified
    Datafile
    /u01/app/oracle/oradata/db1/system01.dbf - dbid changed, wrote new
    name
    Datafile
    /u01/app/oracle/oradata/db1/undotbs01.dbf - dbid changed, wrote new
    name
    Datafile
    /u01/app/oracle/oradata/db1/sysaux01.dbf - dbid changed, wrote new
    name
    Datafile
    /u01/app/oracle/oradata/db1/usertbs.dbf - dbid changed, wrote new
    name
    Datafile
    /u01/app/oracle/oradata/db1/raocl.dbf - dbid changed, wrote new
    name
    Datafile
    /u01/app/oracle/oradata/db1/temp01.dbf - dbid changed, wrote new
    name
    Control
    File
    /u01/app/oracle/product/10.2.0/db1/dbs/control01.ctl - dbid
    changed, wrote new name
    Control
    File
    /u01/app/oracle/product/10.2.0/db1/dbs/control02.ctl - dbid
    changed, wrote new name
    Instance
    shut down
    Database name changed to
    AAA.
    Modify parameter file and generate a new password file before restarting.
    Database ID for database AAA
    changed to 1845742016.
    All previous backups and archived redo logs for this database are
    unusable.
    Database has been shutdown, open
    database with RESETLOGS option.
    Succesfully changed database
    name and
    ID.
    DBNEWID - Completed succesfully.
至于引起这个错误的深层次原因，从之前有过的其他操作猜测，会不会是scn不一致的原因？？如果是这个原因，那或许只要很简单的CKPT就可以了。找时间试验一下。</p>
      <a href="/2009/11/03/nid-00135-of-dbname" class="read_more span2 pull-right">继续阅读……</a>
      <div class="clear"></div>
    </div>
    <div class="clear"></div>
  </div>
  <div class="rule"><hr/></div>
  <div id="post-pagination" class="pagination pagination-centered">
  <ul class="pages nav nav-pills">
    <li>
      <a href="/page15">Previous</a>
    </li>
    <li class="page">
      <a href="/">1</a>
    </li>
    <li class="page">
      <a href="/page2">2</a>
    </li>
    <li class="page">
      <a href="/page3">3</a>
    </li>
    <li class="page">
      <a href="/page4">4</a>
    </li>
    <li class="page">
      <a href="/page5">5</a>
    </li>
    <li class="page">
      <a href="/page6">6</a>
    </li>
    <li class="page">
      <a href="/page7">7</a>
    </li>
    <li class="page">
      <a href="/page8">8</a>
    </li>
    <li class="page">
      <a href="/page9">9</a>
    </li>
    <li class="page">
      <a href="/page10">10</a>
    </li>
    <li class="page">
      <a href="/page11">11</a>
    </li>
    <li class="page">
      <a href="/page12">12</a>
    </li>
    <li class="page">
      <a href="/page13">13</a>
    </li>
    <li class="page">
      <a href="/page14">14</a>
    </li>
    <li class="page">
      <a href="/page15">15</a>
    </li>
    <li class="page active">
      <a href="#">16</a>
    </li>
    <li class="page">
      <a href="/page17">17</a>
    </li>
    <li>
      <a href="/page17">Next</a>
    </li>
  </ul>
</div>
</div>
      </div>
      <div class="span4">
          <div class="well sidebar-nav">
             <ul id="relate_blog" class="nav nav-list">
               <li class="nav-header">最近文章</li>
            </ul>
          </div>
        <div class="well sidebar-nav">
          <iframe width="100%" height="550" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=1&ptype=1&speed=0&skin=2&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=1035836154&verifier=a26926d5&dpc=1"></iframe>
        </div>
        <div class="well sidebar-nav">
            <div id="uyan_list_time_frame"></div>
            <script type="text/javascript" id="UYScriptTime" src="http://v1.uyan.cc/js/iframe_time_list.js?UYUserId=1589850&rankType=time" async=""></script>
        </div>
        <div class="well sidebar-nav">
          <ul id="linklists" class="nav nav-list">
            <li class="nav-header">友情链接(英文)</li>
              <li><a href="http://codeascraft.com/" title="Etsy 运维团队博客">Code as Craft</a></li>
              <li><a href="http://devopsanywhere.blogspot.jp/" title="">devopsanywhere</a></li>
              <li><a href="http://www.jedi.be/blog/" title="">Jong En Dynamische Informatica</a></li>
              <li><a href="http://www.planetdevops.net/" title="">planetdevops</a></li>
              <li><a href="http://www.kitchensoap.com/" title="《网站运维》作者，Etsy 运维">Kitchen Soap</a></li>
              <li><a href="http://blog.johngoulah.com" title="Musings of linux, open source, cloud computing and systems">John Goulah</a></li>
              <li><a href="http://serverfault.com/" title="stackexchange下属的系统工程师问答网站">serverfault</a></li>
              <li><a href="http://www.thegeekstuff.com/" title="各种超酷Linux命令用法">TheGeekStuff</a></li>
              <li><a href="http://neilb.org/" title="The good,the bad,and the beautiful">neilb</a></li>
              <li><a href="http://www.reddit.com/r/perl/" title="">reddit perl 频道</a></li>
              <li><a href="http://jpetazzo.github.io/" title="">~jpetazzo</a></li>
              <li><a href="http://www.perfplanet.com/" title="News and views from the web performance blogosphere">Performance Planet</a></li>
              <li><a href="http://cuddletech.com/blog/" title="Use UNIX or die">Cuddle Tech</a></li>
              <li><a href="http://showmetheco.de/" title="Viacheslav Tykhanovskyi(PocketIO/Text::Haml)">No time to wait</a></li>
              <li><a href="http://blog.dataloop.io/" title="A new SaaS monitoring tool for DevOps & Operations">Dataloop.IO</a></li>
              <li><a href="http://www.ducea.com/" title="">MDLog:/sysadmin</a></li>
              <li><a href="http://planeteria.org/perl6/" title="Perl6 文集">Planet Perl 6</a></li>
          </ul>
        </div>
        <div class="well sidebar-nav">
          <ul id="linklists" class="nav nav-list">
            <li class="nav-header">友情链接(中文)</li>
              <li><a href="http://www.nginxs.com/" title="">eric</a></li>
              <li><a href="http://www.hellodb.net/" title="Ali DBA 张瑞">Hello DBA</a></li>
              <li><a href="http://blog.nosqlfan.com/" title="not only sql信息集散地">NoSQLfan</a></li>
              <li><a href="http://ourmysql.com/" title="">OurMySQL</a></li>
              <li><a href="http://zauc.wordpress.com/" title="">Timo</a></li>
              <li><a href="http://www.liurongxing.com/" title="">刘荣星</a></li>
              <li><a href="http://www.cnadn.net/" title="F5工程师">应用交付学习之路</a></li>
              <li><a href="http://scmbob.org/" title="杭州NSN工程师，shell高人~">扛一肩记忆</a></li>
              <li><a href="http://www.php-oa.com/" title="音悦台技术经理">扶凯</a></li>
              <li><a href="http://www.lark.net.cn/" title="王剑">lark's cloud</a></li>
              <li><a href="http://log.heartoutside.com/" title="HeartOutSide">HeartOutside</a></li>
              <li><a href="http://blog.liulantao.com/" title="刘兰涛">Lax</a></li>
              <li><a href="http://niubie.me/" title="莫言">莫言</a></li>
              <li><a href="http://noops.me/" title="小米运维部">NoOps</a></li>
              <li><a href="http://www.searchtech.pro/" title="">云端分布式搜索技术</a></li>
              <li><a href="http://www.usefulshare.com" title="当当网安全运维">UsefulShare</a></li>
              <li><a href="http://junqili.com/" title="深入研究puppet">纸飞机</a></li>
              <li><a href="http://www.chinaxing.org/" title="">ChinaXing</a></li>
              <li><a href="http://bubbyroom.com/" title="守住每一天">Liuyu's blog</a></li>
              <li><a href="http://blog.aka-cool.net/" title="">Aka.Why</a></li>
              <li><a href="http://blog.l8ii.com/" title="刘侨">LairdNote</a></li>
          </ul>
        </div>
        <div class="well sidebar-nav">
          <ul id="booklists" class="nav nav-list">
          <li class="nav-header">我写的第一本技术书籍</li>
          <li><a href='http://product.china-pub.com/3769604'><img src='http://images.china-pub.com/ebook3765001-3770000/3769604/shupi.jpg' border='0' alt='网站运维技术与实践'/></a></li>
        </div>
      </div>
    </div> <!-- row -->
      <footer>
        <p>&copy; 陈子 2012 
          with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
          and <a href="http://twitter.github.com/bootstrap/" target="_blank">Twitter Bootstrap</a>
        </p>
      </footer>
    </div> <!-- /container -->
    <!-- JiaThis Button BEGIN -->
    <script type="text/javascript">var jiathis_config = {data_track_clickback:true};</script>
    <script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_r.js?move=0&amp;uid=1589850" charset="utf-8"></script>
    <!-- JiaThis Button END -->
    <!-- UJian Button BEGIN -->
    <script type="text/javascript" src="http://v1.ujian.cc/code/ujian.js?type=slide&uid=1589850"></script>
    <!-- UJian Button END -->
  </body>
</html>
